{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# multi_agent/functions_agents.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Type\n",
    "from langchain.tools import BaseTool\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langgraph.prebuilt import ToolExecutor\n",
    "\n",
    "import functions_python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## detect_trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PropertiesCalculateTrend(BaseModel):\n",
    "\tsymbol: Optional[str] = Field(None, description=\"The ticker symbol of the financial instrument to be analyzed.\")\n",
    "\n",
    "\tstart_datetime: Optional[str] = Field(None, description=\"The start timestamp of period over which the analysis is done. \\\n",
    "The format of the date should be in the following format %b-%d-%y %H:%M:%S like this example: May-1-2024 13:27:49\")\n",
    "\tend_datetime: Optional[str] = Field(None, description=\"The end timestamp of period over which the analysis is done. \\\n",
    "The format of the date should be in the following format %b-%d-%y %H:%M:%S like this example: May-1-2024 13:27:49. \\\n",
    "The user can set this parameter to now. In this situation this parameter's value is the current date time.\")\n",
    "\n",
    "\tlookback: Optional[str] = Field(None, description=\"The number of seconds, minutes, hours, days, weeks, months or years to look back for calculating the trend of the given symbol. \\\n",
    "This parameter determines the depth of historical data to be considered in the analysis. The format of this value must obey one of the following examples: 30 seconds, 10 minutes, 2 hours, 5 days, 3 weeks, 2 months and 3 years. \\\n",
    "Either start_datetime along with end_datetime should be specified or lookback should be specified but both cases should not happen simultaneously.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CalculateTrend(BaseTool):\n",
    "\tname = \"detect_trend\"\n",
    "\tdescription = \"\"\"Analyzes the trend of a specified financial instrument over a given time range. \\\n",
    "It is designed primarily for financial data analysis, enabling users to gauge the general direction of a security or market index. \\\n",
    "Whether start_datetime with end_datetime, end_datetime with lookback or lookback parameters could be valued for determining the period over which's trend wants to be detected. \\\n",
    "The function returns a numerical value that indicates the trend intensity and direction within the specified parameters. \\\n",
    "Returns a number between -3 and 3 that represents the trend’s intensity and direction. The value is interpreted as follows: \\\n",
    "\\n -3: strong bearish (downward) trend \\\n",
    "\\n -2: moderate bearish (downward) trend \\\n",
    "\\n -1: mild bearish (downward) trend \\\n",
    "\\n 0: without significant trend \\\n",
    "\\n 3: strong bullish (upward) trend \\\n",
    "\\n 2: moderate bullish (upward) trend \\\n",
    "\\n 1: mild bullish (upward) trend\"\"\"\n",
    "\n",
    "\targs_schema: Type[BaseModel] = PropertiesCalculateTrend\n",
    "\treturn_direct: bool = True\n",
    "\n",
    "\tdef _run(\n",
    "\t\t\tself, symbol: str = None, start_datetime: str = None, end_datetime: str = None, lookback: str = None\n",
    "    ) -> dict:\n",
    "\n",
    "\t\tfunction_arguments = {\n",
    "\t\t\t\t\t\t\"symbol\": symbol,\n",
    "\t\t\t\t\t\t\"start_datetime\": start_datetime,\n",
    "\t\t\t\t\t\t\"end_datetime\": end_datetime,\n",
    "\t\t\t\t\t\t\"lookback\": lookback\n",
    "\t\t\t\t\t\t}\n",
    "\t\t\n",
    "\t\tfc = functions_python.FunctionCalls()\n",
    "\n",
    "\t\t# if state\n",
    "\t\treturn fc.detect_trend(function_arguments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## calculate_sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PropertiesCalculateSR(BaseModel):\n",
    "\tsymbol: Optional[str] = Field(None, description=\"The ticker symbol of the financial instrument to be analyzed.\")\n",
    "\ttimeframe: Optional[str] = Field(None, description=\"Specifies the timeframe of the candlestick chart to be analyzed. \\\n",
    "This parameter defines the granularity of the data used for calculating the levels. The only allowed formats would like 3h, 20min, 1d.\")\n",
    "\tlookback_days: Optional[str] = Field(None, description=\"The number of days to look back for calculating the support and resistance levels. \\\n",
    "This parameter determines the depth of historical data to be considered in the analysis. (e.g. 10 days)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CalculateSR(BaseTool):\n",
    "\tname = \"calculate_sr\"\n",
    "\tdescription = \"\"\"Support and resistance levels represent price points on a chart where the odds favor a pause or reversal of a prevailing trend. \\\n",
    "This function analyzes candlestick charts over a specified timeframe and lookback period to calculate these levels and their respective strengths. \\\n",
    "Returns a dictionary containing five lists, each corresponding to a specific aspect of the calculated support and resistance levels: \\\n",
    "1. levels_prices (list of floats): The prices at which support and resistance levels have been identified. \\\n",
    "2. levels_start_timestamps (list of timestamps) \\\n",
    "3. levels_detect_timestamps (list of timestamps) \\\n",
    "4. levels_end_timestamps (list of timestamps) \\\n",
    "5. levels_scores (list of floats): Scores associated with each level, indicating the strength or significance of the level. Higher scores typically imply stronger levels.\"\"\"\n",
    "\n",
    "\targs_schema: Type[BaseModel] = PropertiesCalculateSR\n",
    "\n",
    "\tdef _run(\n",
    "\t\t\tself, symbol: str = None, timeframe: str = None, lookback_days: str = None\n",
    "    ) -> dict:\n",
    "\t\tparameters = {\n",
    "\t\t\t\t\t\t\"symbol\": symbol,\n",
    "\t\t\t\t\t\t\"timeframe\": timeframe,\n",
    "\t\t\t\t\t\t\"lookback_days\": lookback_days\n",
    "\t\t\t\t\t\t}\n",
    "\t\t\n",
    "\t\tfc = functions_python.FunctionCalls()\n",
    "\n",
    "\t\treturn fc.calculate_sr(parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## calculate_sl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PropertiesCalculateSl(BaseModel):\n",
    "\tsymbol: Optional[str] = Field(None, description=\"The ticker symbol of the financial instrument to be analyzed.\")\n",
    "\n",
    "\tmethod: Optional[str] = Field(None, description=\"shows the method of SL calculation.\")\n",
    "\n",
    "\tdirection: Optional[int] = Field(None, description=\"-1: means the user want to calculate stoploss for a short position. 1: means the user want to calculate stoploss for a long position\")\n",
    "\n",
    "\tlookback: Optional[int] = Field(None, description=\"it is used when the method is set to 'minmax' and shows the number of candles that the SL is calculated based on them.\")\n",
    "\n",
    "\tneighborhood: Optional[int] = Field(None, description=\"A parameter that is used in the swing method to define the range or window within which swings are detected. example: \\\n",
    "If the 'neighborhood' parameter is set to 3, it means that the swing detection is based on considering 3 candles to the \\\n",
    "left and 3 candles to the right of the swing point.\")\n",
    "\n",
    "\tatr_coef: Optional[int] = Field(None, description=\"it is used if the method is 'atr' and shows the coefficient of atr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CalculateSL(BaseTool):\n",
    "\tname = \"calculate_sl\"\n",
    "\tdescription = \"\"\"Stoploss (SL) is a limitation for potential losses in a position. It's below the current price for long position and above it for short position. \\\n",
    "Distance between the SL and current price is named risk value. This function calculates the SL based o some different methods. \\\n",
    "Returns A dictionary same as this: \\\n",
    "{'sl': [17542.5], 'risk': [268.5], 'info': ['calculated based on maximum high price of previous 100 candles']} \\\n",
    "which includes sl value, risk on the trade and an information. \\\n",
    "If user don't select any method for sl calculation or select \"level\" method, or zigzag method the otput can include \\\n",
    "more than one stoploss and the values type in the output can be a list such as this \\\n",
    "{'sl': [17542.5, 17818.25, 17858.5, 17882.5, 18518.75], 'risk': [268.5, 7.25, 47.5, 71.5, 707.75], 'info': ['minmax', 'swing', 'atr', '5min_SR', 'daily_SR']} \\\n",
    "It includes a list of stoplosses and the risk on them and finally the level or method name of stoploss.\"\"\"\n",
    "\n",
    "\targs_schema: Type[BaseModel] = PropertiesCalculateSl\n",
    "\n",
    "\tdef _run(\n",
    "        self, symbol: str = None, method: str = None, direction: int = None,\n",
    "\t\tlookback: int = None, neighborhood: int = None, atr_coef: int = None\n",
    "    ) -> int:\n",
    "\t\tparameters = {\n",
    "                      \"symbol\": symbol,\n",
    "                      \"method\": method,\n",
    "                      \"direction\": direction,\n",
    "                      \"lookback\": lookback,\n",
    "                      \"neighborhood\": neighborhood,\n",
    "                      \"atr_coef\": atr_coef\n",
    "                      }\n",
    "\t\t\n",
    "\t\tfc = functions_python.FunctionCalls()\n",
    "\n",
    "\t\treturn fc.calculate_sl(parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## calculate_tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PropertiesCalculateTp(BaseModel):\n",
    "\tsymbol: Optional[str] = Field(None, description=\"The ticker symbol of the financial instrument to be analyzed.\")\n",
    "\n",
    "\tdirection: Optional[int] = Field(None, description=\"-1: means the user want to calculate stoploss for a short position. 1: means the user want to calculate stoploss for a long position\")\n",
    "\n",
    "\tstoploss: Optional[int] = Field(None, description=\"the value for stoploss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CalculateTp(BaseTool):\n",
    "\tname = \"calculate_tp\"\n",
    "\tdescription = \"\"\"Take profit (TP) is opposite of the stop-loss (SL) and is based on maximum reward that we intend to achieve from a trade. \\\n",
    "It represents the price level at which a trader aims to close a position to secure profits before the market reverses. \\\n",
    "Returns list of price for take-profit and information for each price For exampe: \\\n",
    "{'tp': [5139.25, 5140.25, 5144.0], 'info': ['calculated based on the level VWAP_Top_Band_2', 'calculated based on the level Overnight_high', 'calculated based on the level VWAP_Top_Band_3']}\"\"\"\n",
    "\n",
    "\targs_schema: Type[BaseModel] = PropertiesCalculateTp\n",
    "\n",
    "\tdef _run(\n",
    "        self, symbol: str = None, direction: int = None, stoploss: int = None\n",
    "    ) -> int:\n",
    "\t\tparameters = {\n",
    "\t\t\t\t\t\t\"symbol\": symbol,\n",
    "\t\t\t\t\t\t\"direction\": direction,\n",
    "\t\t\t\t\t\t\"stoploss\": stoploss\n",
    "\t\t\t\t\t\t}\n",
    "\t\t\n",
    "\t\tfc = functions_python.FunctionCalls()\n",
    "\n",
    "\t\treturn fc.calculate_tp(parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PropertiesBiasDetection(BaseModel):\n",
    "\tsymbol: Optional[str] = Field(None, description=\"The ticker symbol of the financial instrument to be analyzed.\")\n",
    "\n",
    "\tmethod: Optional[str] = Field(None, description=\"The user can choose from different methods including MC, Zigzag trend, \\\n",
    "Trend detection, weekly wvap, candle stick pattern, cross ma, vp detection ,power & counter ratio.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiasDetection(BaseTool):\n",
    "\tname = \"bias_detection\"\n",
    "\tdescription = \"\"\"Detecting trading bias through different methods or Detecting the appropriate entry point for a long or short trade.\n",
    "Returns a number between -3 and 3 that represents the trend’s intensity and direction. The value is interpreted as follows:\n",
    "-3: Strong downward , -2: downward -1: Weak downward, 0: No significant trend / Neutral, 1: Weak upward, 2.upward, 3: Strong upward\"\"\"\n",
    "\n",
    "\targs_schema: Type[BaseModel] = PropertiesBiasDetection\n",
    "\n",
    "\tdef _run(\n",
    "        self, symbol: str = None, method: str = None\n",
    "    ) -> dict:\n",
    "\n",
    "\t\tparameters = {\n",
    "\t\t\t\t\t\t\"symbol\": symbol,\n",
    "\t\t\t\t\t\t\"method\": method\n",
    "\t\t\t\t\t\t}\n",
    "\t\t\n",
    "\t\tfc = functions_python.FunctionCalls()\n",
    "\n",
    "\t\treturn fc.get_bias(parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## irrelevant_handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Handlers:\n",
    "\tdef __init__(self, client, ChatWithOpenai):\n",
    "\t\tself.client = client\n",
    "\t\tself.default_message = [\"Check the following text for greeting words and do as the system message said.\"]\n",
    "\t\tself.ChatWithOpenai = ChatWithOpenai\n",
    "\n",
    "\tdef handler_zero(self):\n",
    "\t\thandler_zero_openai = self.ChatWithOpenai(system_message=\"You are an assistant. Your job is to check the user input Not answer it.\\\n",
    "If the user input contains greeting words like ‘hello’, ‘hi’, and so on, \\\n",
    "then you should remove these words. \\\n",
    "Note that the final response is either the input with the greeting word removed, \\\n",
    "or ‘None’ if the input consists only of a greeting word. \\\n",
    "if there is no greeting word in the input, the response is the last user input itself \\\n",
    "without any changes. \\\n",
    "No other responses are possible. \\\n",
    "Here are some examples and the valid responses: \\\n",
    "Example 1 (when there is no greeting word): \\\n",
    "{ input: ‘What is the weather?’ response: ‘What is the weather?’ } \\\n",
    "Example 2 (when there is a greeting word): \\\n",
    "{ input: ‘Hi can you speak French?’ response: ‘Can you speak French?’ } \\\n",
    "Example 3 (when there is just one or more greeting words): \\\n",
    "{ input: ‘Hello.’ response: ‘None’ } \\\n",
    "Note that you are not permitted to answer user requests directly. \\\n",
    "Only perform the tasks instructed by system messages.\",\n",
    "\n",
    "\t\t\t\t\t\t\t\t\t\t\tdefault_user_messages=self.default_message,\n",
    "\t\t\t\t\t\t\t\t\t\t\t# model=\"gpt_35_16k\",\n",
    "\t\t\t\t\t\t\t\t\t\t\ttemperature=0,\n",
    "\t\t\t\t\t\t\t\t\t\t\tmax_tokens=100,\n",
    "\t\t\t\t\t\t\t\t\t\t\t# client=self.client\n",
    "\t\t\t\t\t\t\t\t\t\t\t)\n",
    "\t\treturn handler_zero_openai\n",
    "\n",
    "\tdef handler_one(self):\n",
    "\t\thandler_one_openai = self.ChatWithOpenai(system_message=\"You are an assistant. Your job is to check the user input, not answer it. \\\n",
    "We are a system with the name 'Tensurf Brain' or 'Tensurf'. \\\n",
    "If the user needs any information about us or needs a tutorial for \\\n",
    "how our system is working, your job is to detect these scenarios and respond with 'True'. \\\n",
    "If the user asks about you, the answer is also 'True'. \\\n",
    "Also, when the user is confused and doesn't know how to start or what to do, \\\n",
    "the answer is also 'True'. \\\n",
    "For any other input that doesn't classify in the tutorial or information \\\n",
    "about 'Tensurf Brain' or 'Tensurf' you should return false. \\\n",
    "Note that you are not permitted to answer user requests directly. \\\n",
    "Only perform the tasks instructed by system messages.\",\n",
    "\t\t\t\t\t\t\t\t\t\t\t# model=\"gpt_35_16k\",\n",
    "\t\t\t\t\t\t\t\t\t\t\ttemperature=0,\n",
    "\t\t\t\t\t\t\t\t\t\t\tmax_tokens=100,\n",
    "\t\t\t\t\t\t\t\t\t\t\t# client=self.client\n",
    "\t\t\t\t\t\t\t\t\t\t\t)\n",
    "\t\treturn handler_one_openai\n",
    "\n",
    "\tdef handler_two(self):\n",
    "\t\thandler_two_openai = self.ChatWithOpenai(system_message=\"You are an assistant. Your job is to check the user input. \\\n",
    "If the user input does not contain any financial and \\\n",
    "trading topics or requests, then you should answer only\\\n",
    "with ‘True’. Otherwise, return ‘False’. \\\n",
    "Possible responses for you are 'True' or 'False'. \\\n",
    "For example, the correct response to the question \\\n",
    "'What is the trend?' is 'False'.\",\n",
    "\t\t\t\t\t\t\t\t\t# model=\"gpt_35_16k\",\n",
    "\t\t\t\t\t\t\t\t\ttemperature=0,\n",
    "\t\t\t\t\t\t\t\t\tmax_tokens=100,\n",
    "\t\t\t\t\t\t\t\t\t# client=self.client\n",
    "\t\t\t\t\t\t\t\t\t)\n",
    "\t\treturn handler_two_openai\n",
    "\n",
    "\n",
    "def create_irrelavant_handler(client, ChatWithOpenai):\n",
    "\t######## Irrelevant Handler ########\n",
    "\tclass HandleIrrelevantSchema(BaseModel):\n",
    "\t\tmassage: str = Field(..., description=\"The humanmassage\")\n",
    "\n",
    "\tclass HandleIrrelevant(BaseTool):\n",
    "\t\tname = \"HandleIrrelevant\"\n",
    "\t\tdescription = \"\"\"This function checks if the message contains financial or trading subjects or not. \\\n",
    "\tThe output of this function is either True or False and the possible output of this function are 'True' or 'False'.: \\\n",
    "\tTrue: when the message contains financial or trading subjects. \\\n",
    "\tFalse: when the message request is not in these fields.\"\"\"\n",
    "\n",
    "\t\targs_schema: Type[BaseModel] = HandleIrrelevantSchema\n",
    "\n",
    "\t\tdef _run(\n",
    "\t\t\tself, massage: str\n",
    "\t\t) -> dict:\n",
    "\n",
    "\t\t\thandlers = Handlers(client=client, ChatWithOpenai=ChatWithOpenai)\n",
    "\t\t\tuser_input = [{\"role\": \"user\", \"content\": \"'\" + massage + \"'\"}]\n",
    "\n",
    "\t\t\thandler_zero_openai = handlers.handler_zero()\n",
    "\t\t\thandler_one_openai = handlers.handler_one()\n",
    "\t\t\thandler_two_openai = handlers.handler_two()\n",
    "\n",
    "\t\t\tmodified_input = handler_zero_openai.chat(user_input)\n",
    "\t\t\tif modified_input == 'None':\n",
    "\t\t\t\treturn 'Greeting'\n",
    "\n",
    "\t\t\tmodified_user_input = [{\"role\": \"user\", \"content\": modified_input}]\n",
    "\n",
    "\t\t\tif handler_one_openai.chat(modified_user_input) == 'True':\n",
    "\t\t\t\treturn \"Tutorial\"\n",
    "\t\t\telse:\n",
    "\t\t\t\treturn handler_two_openai.chat(modified_user_input)\n",
    "\t\n",
    "\tHandler = HandleIrrelevant()\n",
    "\treturn Handler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## creating tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_agent_tools(client, ChatWithOpenai):\n",
    "\n",
    "\tTrend = CalculateTrend()\n",
    "\tSR = CalculateSR()\n",
    "\tSL = CalculateSL()\n",
    "\tTP = CalculateTp()\n",
    "\tBias = BiasDetection()\n",
    "\tHandler = create_irrelavant_handler(client, ChatWithOpenai)\n",
    "\n",
    "\ttools = [Trend, SR, TP, SL, Bias, Handler]\n",
    "\ttool_executor = ToolExecutor(tools)\n",
    "\ttrading_tools = [Trend, SR, SL, TP, Bias]\n",
    "\t\n",
    "\treturn tool_executor, trading_tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# multi_agent/utils.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import operator\n",
    "from typing import Annotated, Sequence, TypedDict\n",
    "from openai import AzureOpenAI\n",
    "from langchain.tools.render import format_tool_to_openai_function\n",
    "from langchain_core.utils.function_calling import convert_to_openai_function\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import FunctionMessage, HumanMessage\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langgraph.prebuilt.tool_executor import ToolInvocation\n",
    "\n",
    "from multi_agent.functions_agents import create_agent_tools\n",
    "import input_filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Utils:\n",
    "    def __init__(self, ChatWithOpenai, client):\n",
    "        self.api_type = \"azure\"\n",
    "        self.api_endpoint = 'https://tensurfbrain1.openai.azure.com/'\n",
    "        self.api_version = '2023-10-01-preview'\n",
    "        self.api_key = '80ddd1ad72504f2fa226755d49491a61'\n",
    "        self.llm = AzureChatOpenAI(\n",
    "                        api_key=self.api_key,\n",
    "                        api_version=self.api_version,\n",
    "                        azure_endpoint=self.api_endpoint,\n",
    "                        deployment_name=\"gpt_35_16k\",\n",
    "                        temperature=0,\n",
    "                        streaming=True\n",
    "        )\n",
    "        self.client = client\n",
    "        self.ChatWithOpenai = ChatWithOpenai\n",
    "\n",
    "    def create_agent(self, llm, tools, system_message: str):\n",
    "        \"\"\"Create an agent.\"\"\"\n",
    "        functions = [convert_to_openai_function(t) for t in tools]\n",
    "        prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\n",
    "                    \"system\",\n",
    "                    \" You are a helpful AI assistant, collaborating with other assistants.\"\n",
    "                    \" Use the provided tools to progress towards answering the question.\"\n",
    "                    \" If you are unable to fully answer, that's OK, another assistant with different tools \"\n",
    "                    \" will help where you left off. Execute what you can to make progress.\"\n",
    "                    \" If you or any of the other assistants have the final answer or deliverable,\"\n",
    "                    \" prefix your response with FINAL ANSWER so the team knows to stop.\"\n",
    "                    f\" You have access to the following tools: {tool}.\\n{system_message}\",\n",
    "                ),\n",
    "                MessagesPlaceholder(variable_name=\"messages\"),\n",
    "            ]\n",
    "        )\n",
    "        prompt = prompt.partial(system_message=system_message)\n",
    "        prompt = prompt.partial(tool_names=\", \".join([tool.name for tool in tools]))\n",
    "        return prompt | llm.bind_functions(functions)\n",
    "\n",
    "    def agent_node(self, state, agent, name):\n",
    "        result = agent.invoke(state)\n",
    "\n",
    "        if isinstance(result, FunctionMessage):\n",
    "            pass\n",
    "\n",
    "        else:\n",
    "            result = HumanMessage(**result.dict(exclude={\"type\", \"name\"}), name=name)\n",
    "\n",
    "        return {\n",
    "            \"messages\": [result],\n",
    "            \"sender\": name,\n",
    "            \"output_json\": state[\"output_json\"]\n",
    "        }\n",
    "\n",
    "    def run_Handler(self, state):\n",
    "        tool_name = \"HandleIrrelevant\"\n",
    "\n",
    "        messages = state[\"messages\"]\n",
    "        last_message = messages[-1]\n",
    "\n",
    "        # print(\"&\"*50)\n",
    "        # print(last_message.content)\n",
    "        # print(\"&\"*50)\n",
    "\n",
    "        action = ToolInvocation(\n",
    "            tool=tool_name,\n",
    "            tool_input=last_message.content,\n",
    "        )\n",
    "\n",
    "        tool_executor, _ = create_agent_tools(client=self.client, ChatWithOpenai=self.ChatWithOpenai)\n",
    "        response = tool_executor.invoke(action)\n",
    "        # print(response)\n",
    "        standard_response = \"\"\n",
    "        if response == 'True':\n",
    "            standard_response = \"I'm here to help with trading and financial market queries. If you think your ask relates to trading and isn't addressed, please report a bug using the bottom right panel.\"\n",
    "\n",
    "        function_message = FunctionMessage(\n",
    "            content=standard_response, HandleIrrelevant_response=f\"{str(response)}\", name=action.tool\n",
    "        )\n",
    "\n",
    "        return {\"messages\": [function_message]}\n",
    "\n",
    "    def run_greeting(self, state):\n",
    "        greeting = self.ChatWithOpenai(system_message=\"You are an financial and trading assistant.\",\n",
    "                                # model=\"gpt_35_16k\",\n",
    "                                temperature=0,\n",
    "                                max_tokens=100,\n",
    "                                # client=self.client\n",
    "                                )\n",
    "        input = [{\"role\": \"user\", \"content\": \"Hi\"}]\n",
    "        response = greeting.chat(input)\n",
    "\n",
    "        function_message = FunctionMessage(\n",
    "            content=response ,name='openai chat'\n",
    "        )\n",
    "\n",
    "        return {\"messages\": [function_message]}\n",
    "\n",
    "    def run_tutorial(self, state):\n",
    "        response = \"\"\"I'm TenSurf Brain, your AI trading assistant within TenSurf Hub platform, designed to enhance your trading experience with advanced analytical and data-driven tools: \\\n",
    "1. Trend Detection: I can analyze and report the trend of financial instruments over your specified period. For example, ask me, \"What is the trend of NQ stock from May-1-2024 12:00:00 until May-5-2024 12:00:00?\" \\\n",
    "2. Support and Resistance Levels: I identify and score key price levels that may influence market behavior based on historical data. Try, \"Calculate Support and Resistance Levels based on YM by looking back up to the past 10 days and a timeframe of 1 hour.\" \\\n",
    "3. Stop Loss Calculation: I determine optimal stop loss points to help you manage risk effectively. Query me like, \"How much would be the optimal stop loss for a short trade on NQ?\" \\\n",
    "4. Take Profit Calculation: I calculate the ideal exit points for securing profits before a potential trend reversal. For example, \"How much would be the take-profit of a short position on Dow Jones with the stop loss of 10 points?\" \\\n",
    "5. Trading Bias Identification: I analyze market conditions to detect the best trading biases and directions, whether for long or short positions. Ask me, \"What is the current trading bias for ES?\" \\\n",
    "Each tool is tailored to help you make smarter, faster, and more informed trading decisions. Enjoy!\"\"\"\n",
    "\n",
    "        function_message = FunctionMessage(\n",
    "            content=response ,name='hardcoded string'\n",
    "        )\n",
    "\n",
    "        return {\"messages\": [function_message]}\n",
    "\n",
    "    def output_json_assigner(self, tool_name, response, symbol, input_json):\n",
    "        output_json = {}\n",
    "        if tool_name == \"calculate_sr\":\n",
    "            sr_value, sr_start_date, sr_detect_date, sr_end_date, sr_importance = response\n",
    "            output_json[\"levels_prices\"] = sr_value\n",
    "            output_json[\"levels_start_timestamps\"] = sr_start_date\n",
    "            output_json[\"levels_detect_timestamps\"] = sr_detect_date\n",
    "            output_json[\"levels_end_timestamps\"] = sr_end_date\n",
    "            output_json[\"levels_scores\"] = sr_importance\n",
    "            output_json[\"function_name\"] = tool_name\n",
    "            output_json[\"symbol\"] = symbol\n",
    "            output_json[\"timeframe\"] = input_json[\"timeframe\"]\n",
    "        return output_json\n",
    "\n",
    "    def tool_node(self, state):\n",
    "        \"\"\"This runs tools in the graph\n",
    "            It takes in an agent action\n",
    "            and calls that tool and\n",
    "            returns the result.\"\"\"\n",
    "        messages = state[\"messages\"]\n",
    "        last_message = messages[-1]\n",
    "        tool_input = json.loads(\n",
    "            last_message.additional_kwargs[\"function_call\"][\"arguments\"]\n",
    "        )\n",
    "        output_json = {}\n",
    "\n",
    "        if len(tool_input) == 1 and \"__arg1\" in tool_input:\n",
    "            tool_input = next(iter(tool_input.values()))\n",
    "\n",
    "        tool_name = last_message.additional_kwargs[\"function_call\"][\"name\"].split(\".\")[-1]\n",
    "\n",
    "        action = ToolInvocation(\n",
    "            tool=tool_name,\n",
    "            tool_input=tool_input,\n",
    "        )\n",
    "\n",
    "        tool_input = input_filter.input_filter(tool_name, tool_input, state[\"input_json\"])\n",
    "        \n",
    "        if tool_name == \"detect_trend\":\n",
    "            tool_input, results, correct_dates = tool_input\n",
    "            state[\"input_json\"].update(tool_input)\n",
    "            if not correct_dates:\n",
    "                function_message = FunctionMessage(\n",
    "                    content=f\"{tool_name} response: {results}\", name=action.tool\n",
    "                )\n",
    "                return {\"messages\": [function_message], \"output_json\":output_json}\n",
    "        else:\n",
    "            state[\"input_json\"].update(tool_input)\n",
    "\n",
    "        tool_executor, _ = create_agent_tools(client=self.client, ChatWithOpenai=self.ChatWithOpenai)\n",
    "        response = tool_executor.invoke(action)\n",
    "        symbol = tool_input[\"symbol\"]\n",
    "        output_json = self.output_json_assigner(tool_name, response, symbol, state[\"input_json\"])\n",
    "\n",
    "        function_message = FunctionMessage(\n",
    "            content=f\"{tool_name} response: {str(response)}\", name=action.tool\n",
    "        )\n",
    "\n",
    "        return {\"messages\": [function_message], \"output_json\":output_json}\n",
    "\n",
    "    def router(self, state):\n",
    "        messages = state[\"messages\"]\n",
    "        last_message = messages[-1]\n",
    "\n",
    "        if last_message.name != 'HandleIrrelevant':\n",
    "            last_message = messages[-2]\n",
    "\n",
    "            if \"function_call\" in last_message.additional_kwargs:\n",
    "                return \"call_tool\"\n",
    "\n",
    "            if not \"function_call\" in last_message.additional_kwargs and last_message.type == \"function\":\n",
    "                return \"continue\"\n",
    "\n",
    "            if not last_message.additional_kwargs and last_message.type == \"human\":\n",
    "                return \"end\"\n",
    "\n",
    "        else:\n",
    "            if \"Greeting\" in last_message.HandleIrrelevant_response and last_message.name == 'HandleIrrelevant':\n",
    "                return \"Greeting\"\n",
    "\n",
    "            if \"Tutorial\" in last_message.HandleIrrelevant_response and last_message.name == 'HandleIrrelevant':\n",
    "                return \"Tutorial\"\n",
    "\n",
    "            if \"True\" in last_message.HandleIrrelevant_response and last_message.name == 'HandleIrrelevant':\n",
    "                return \"end\"\n",
    "\n",
    "            if \"False\" in last_message.HandleIrrelevant_response and last_message.name == 'HandleIrrelevant':\n",
    "                return \"continue\"\n",
    "\n",
    "            if \"False\" in last_message.content and last_message.name == \"HandleGreeting\":\n",
    "                return \"continue\"\n",
    "\n",
    "            if \"False\" not in last_message.content and last_message.name == \"HandleGreeting\":\n",
    "                return \"end\"\n",
    "\n",
    "        return \"continue\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AgentState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "    output_json: dict\n",
    "    input_json: dict\n",
    "    sender: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# multi_agent/multi_agent.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "from multi_agent.utils import Utils, AgentState\n",
    "from multi_agent.functions_agents import create_agent_tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multi_Agent:\n",
    "    def __init__(self, ChatWithOpenai, client):\n",
    "        self.output_json = {}\n",
    "        self.ChatWithOpenai = ChatWithOpenai\n",
    "        self.client = client\n",
    "        self.utils = Utils(self.ChatWithOpenai, self.client)\n",
    "\n",
    "    def initialize_graph(self):\n",
    "        _, trading_tools_list = create_agent_tools(client=self.client, ChatWithOpenai=self.ChatWithOpenai)\n",
    "        trading_agent = self.utils.create_agent(\n",
    "            self.utils.llm,\n",
    "            trading_tools_list,\n",
    "            system_message=\"\"\"You are an assistant with the following capabilities given to you by your tools:\n",
    "SR: Calculate support and resistance levels. \\\n",
    "Trend: Calculate the trend of a specified financial instrument over a given time range and timeframe. \\\n",
    "TP: Calculate Take profit (TP), \\\n",
    "SL: Calculate Stoploss (SL), \\\n",
    "Bias: Detecting trading bias. \\\n",
    "Note the following rules for result of each tool: \\\n",
    "SR:Do not mention the name of the parameters of the functions directly in the final answer. Instead, briefly explain them and use other meaningfuly related synonyms. Do not mention the name of the levels that the level is support or resistance. The final answer should also contain the following texts: These levels are determined based on historical price data and indicate areas where the price is likely to encounter support or resistance. The associated scores indicate the strength or significance of each level, with higher scores indicating stronger levels. \\\n",
    "Trend:At any situations, never return the number which is the output of the Trend function. Instead, use its correcsponding explanation which is in the Trend tool's description. Make sure to mention the start_datetime and end_datetime or the lookback parameter. Do not mention the name of the parameters of the functions directly in the final answer. Instead, briefly explain them and use other meaningfuly related synonyms. Now generate a proper response. \\\n",
    "TP: Do not mention the name of the parameters of the functions directly in the final answer. Instead, briefly explain them and use other meaningfuly related synonyms. Now generate a proper response. \\\n",
    "SL: Do not mention the name of the parameters of the functions directly in the final answer. Instead, briefly explain them and use other meaningfuly related synonyms. The unit of every number in the answer should be mentioned. Now generate a proper response. \\\n",
    "Bias: Do not mention the name of the parameters of the functions directly in the final answer. Instead, briefly explain them and use other meaningfuly related synonyms. Now generate a proper response.\"\"\"\n",
    "        )\n",
    "        trading_node = functools.partial(self.utils.agent_node, agent=trading_agent, name=\"Trading\")\n",
    "\n",
    "        workflow = StateGraph(AgentState)\n",
    "\n",
    "        workflow.add_node(\"Trading\", trading_node)\n",
    "        workflow.add_node(\"call_tool\", self.utils.tool_node)\n",
    "        workflow.add_node(\"Handler\", self.utils.run_Handler)\n",
    "        workflow.add_node(\"Greeting\", self.utils.run_greeting)\n",
    "        workflow.add_node(\"Tutorial\", self.utils.run_tutorial)\n",
    "\n",
    "        workflow.add_edge(\"Greeting\", END)\n",
    "        workflow.add_edge(\"Tutorial\", END)\n",
    "\n",
    "        workflow.add_conditional_edges(\n",
    "            \"Handler\",\n",
    "            self.utils.router,\n",
    "            {\"continue\": \"Trading\", \"Greeting\": \"Greeting\", \"Tutorial\": \"Tutorial\", \"end\": END},\n",
    "        )\n",
    "\n",
    "        workflow.add_conditional_edges(\n",
    "            \"Trading\",\n",
    "            self.utils.router,\n",
    "            {\"continue\": \"Trading\", \"call_tool\": \"call_tool\", \"end\": END},\n",
    "        )\n",
    "\n",
    "        workflow.add_conditional_edges(\n",
    "            \"call_tool\",\n",
    "            lambda x: x[\"sender\"],\n",
    "            {\n",
    "                \"Trading\": \"Trading\"\n",
    "            },\n",
    "        )\n",
    "        workflow.set_entry_point(\"Handler\")\n",
    "\n",
    "        graph = workflow.compile()\n",
    "\n",
    "        return graph\n",
    "\n",
    "    def generate_multi_agent_answer(self, input_json, graph):\n",
    "        generated_messages = graph.stream(\n",
    "            {\n",
    "                \"messages\": [\n",
    "                    HumanMessage(\n",
    "                        content=input_json[\"new_message\"]\n",
    "                    )\n",
    "                ],\n",
    "                \"input_json\": input_json\n",
    "            },\n",
    "            # Maximum number of steps to take in the graph\n",
    "            {\"recursion_limit\": 150},\n",
    "        )\n",
    "        \n",
    "        generated_messages = list(generated_messages)\n",
    "        k = list(generated_messages[-1].keys())[0]\n",
    "        if \"output_json\" in generated_messages[-1][k]:\n",
    "            self.output_json = {\n",
    "                \"response\": generated_messages[-1][k][\"messages\"][0].content,\n",
    "                \"chart_info\": generated_messages[-1][k][\"output_json\"]\n",
    "            }\n",
    "        else:\n",
    "            self.output_json = {\n",
    "                \"response\": generated_messages[-1][k][\"messages\"][0].content\n",
    "            }\n",
    "        \n",
    "        return self.output_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI, AzureOpenAI\n",
    "from groq import Groq\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from file_processor import FileProcessor\n",
    "from single_agent import Single_Agent\n",
    "from multi_agent import Multi_Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "DEBUG = (os.getenv('DEBUG', 'True') == 'True')\n",
    "\n",
    "# class ChatWithOpenai:\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         system_message,\n",
    "#         model,\n",
    "#         temperature,\n",
    "#         max_tokens,\n",
    "#         client,\n",
    "#         default_user_messages=None,\n",
    "#     ):\n",
    "#         self.system_message = system_message\n",
    "#         self.model = model\n",
    "#         self.temperature = temperature\n",
    "#         self.max_tokens = max_tokens\n",
    "#         azure_connectto_surf = AzureConnecttoSurf()\n",
    "#         self.client = azure_connectto_surf.client\n",
    "#         self.messages = [{\"role\": \"system\", \"content\": system_message}]\n",
    "#         if default_user_messages:\n",
    "#             for user_message in default_user_messages:\n",
    "#                 self.messages += [{\"role\": \"user\", \"content\": user_message}]\n",
    "\n",
    "#     def chat(self, user_input):\n",
    "#         response = self.client.chat.completions.create(\n",
    "#             model=self.model,\n",
    "#             messages=self.messages + user_input,\n",
    "#             temperature=self.temperature,\n",
    "#             max_tokens=self.max_tokens,\n",
    "#         )\n",
    "#         return response.choices[0].message.content\n",
    "\n",
    "\n",
    "class ChatWithOpenai:\n",
    "    def __init__(self, system_message, temperature=0, max_tokens=4096, default_user_messages=None):\n",
    "        groqconnecttosurf = GroqConnecttoSurf()\n",
    "        self.system_message = system_message\n",
    "        self.models = groqconnecttosurf.models\n",
    "        self.temperature = temperature\n",
    "        self.max_tokens = max_tokens\n",
    "        self.clients = groqconnecttosurf.clients\n",
    "        self.messages = [{\"role\": \"system\", \"content\": system_message}]\n",
    "        if default_user_messages:\n",
    "            for user_message in default_user_messages:\n",
    "                self.messages += [{\"role\": \"user\", \"content\": user_message}]\n",
    "\n",
    "    def chat(self, user_input):\n",
    "        for client, model in zip(self.clients, self.models):\n",
    "            try:\n",
    "                response = client.chat.completions.create(\n",
    "                    model=model,\n",
    "                    messages=self.messages + user_input,\n",
    "                    temperature=self.temperature,\n",
    "                    max_tokens=self.max_tokens,\n",
    "                    stream=bool(int(os.getenv(\"stream\"))),\n",
    "                )\n",
    "                return response.choices[0].message.content\n",
    "            except Exception as e:\n",
    "                print(f\"Error with client: client{self.clients.index(client)}. Exception: {e}\")\n",
    "\n",
    "\n",
    "class GroqConnecttoSurf:\n",
    "    def __init__(self) -> None:\n",
    "        self.models = [os.getenv(\"MODEL1\"), os.getenv(\"MODEL1\"), os.getenv(\"MODEL1\"), os.getenv(\"MODEL2\"), os.getenv(\"MODEL3\")]\n",
    "        client1 = Groq(api_key=os.getenv(\"groq_api1\"))\n",
    "        client2 = Groq(api_key=os.getenv(\"groq_api2\"))\n",
    "        client3 = Groq(api_key=os.getenv(\"groq_api3\"))\n",
    "        client4 = AzureOpenAI(\n",
    "            api_key=os.getenv(\"api_key1\"),\n",
    "            api_version=os.getenv(\"azure_api_version\"),\n",
    "            azure_endpoint=os.getenv(\"azure_api_endpoint\")\n",
    "            )\n",
    "        client5 = AzureOpenAI(\n",
    "            api_key=os.getenv(\"api_key2\"),\n",
    "            api_version=os.getenv(\"azure_api_version\"),\n",
    "            azure_endpoint=os.getenv(\"azure_api_endpoint\")\n",
    "            )\n",
    "        self.clients = [client1, client2, client3, client4, client5]\n",
    "\n",
    "\n",
    "class AzureConnecttoSurf:\n",
    "    def __init__(self) -> None:\n",
    "        self.api_endpoint = os.getenv(\"azure_api_endpoint\")\n",
    "        self.api_version = os.getenv(\"azure_api_version\")\n",
    "        self.api_key = os.getenv(\"azure_api_key\")\n",
    "        self.client = AzureOpenAI(\n",
    "            api_key=self.api_key,\n",
    "            api_version=self.api_version,\n",
    "            azure_endpoint=self.api_endpoint,\n",
    "        )\n",
    "        self.tts_api_version = os.getenv(\"tts_api_version\")\n",
    "        self.tts_model1 = os.getenv(\"tts_model1\")\n",
    "        self.GPT_MODEL = os.getenv(\"azure_GPT_MODEL_3\")\n",
    "        self.whisper_model = os.getenv(\"azure_whisper_model\")\n",
    "        self.voice_name = os.getenv(\"voice_name\")\n",
    "        self.tts_model = os.getenv(\"tts_model2\")\n",
    "\n",
    "\n",
    "class OpenaiConnecttoSurf:\n",
    "    def __init__(self) -> None:\n",
    "        self.client = OpenAI(api_key=os.getenv(\"openai_api_key\"))\n",
    "        self.GPT_MODEL = os.getenv(\"openai_GPT_MODEL_3\")\n",
    "        self.whisper_model = os.getenv(\"whisper_model_openai\")\n",
    "\n",
    "\n",
    "def check_relevance(connector_surf, prompt: str):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"Classify if the following prompt is relevant or irrelevant. Guidelines for you as a Trading Assistant:Relevance: Focus exclusively on queries related to trading and financial markets(including stock tickers). If a question falls outside this scope, politely inform the user that the question is beyond the service's focus.Accuracy: Ensure that the information provided is accurate and up-to-date. Use reliable financial data and current market analysis to inform your responses.Clarity: Deliver answers in a clear, concise, and understandable manner. Avoid jargon unless the user demonstrates familiarity with financial terms.Promptness: Aim to provide responses quickly to facilitate timely decision-making for users.Confidentiality: Do not ask for or handle personal investment details or sensitive financial information.Compliance: Adhere to legal and ethical standards applicable to financial advice and information dissemination.Again, focus solely on topics related to trading and financial markets. Politely notify the user if a question is outside this specific area of expertise.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "    response = connector_surf.client.chat.completions.create(\n",
    "        model=connector_surf.GPT_MODEL, temperature=0, messages=messages\n",
    "    )\n",
    "    relevance_check = response.choices[0].message.content\n",
    "    if \"irrelevant\" in relevance_check.lower():\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "# class llm_surf:\n",
    "def llm_surf(llm_input: dict) -> str:\n",
    "\n",
    "    llm_output = {\n",
    "        \"response\": \"\",\n",
    "        \"symbol\": llm_input.get(\"symbol\"),\n",
    "        \"file\": None,\n",
    "        \"function_call\": None,\n",
    "    }\n",
    "\n",
    "    azure_connector_surf = AzureConnecttoSurf()\n",
    "\n",
    "    content = \"\"\n",
    "    fileProcessor = FileProcessor(azure_connector_surf)\n",
    "    if \"file\" in llm_input and llm_input[\"file\"]:\n",
    "        if type(llm_input[\"file\"]) == str:\n",
    "            llm_input[\"file\"] = open(llm_input[\"file\"], \"r\")\n",
    "        file_content = fileProcessor.get_file_content(llm_input[\"file\"])\n",
    "        if file_content:\n",
    "            content += file_content + \"\\n\"\n",
    "\n",
    "    if llm_input.get(\"new_message\"):\n",
    "        content += llm_input.get(\"new_message\") + \"\\n\"\n",
    "    else:\n",
    "        content += fileProcessor.default_prompt + \"\\n\"\n",
    "\n",
    "    # running in multi-agent mode\n",
    "    if os.getenv(\"MODE\") == \"multi-agent\":\n",
    "        MA = Multi_Agent(\n",
    "            ChatWithOpenai=ChatWithOpenai, client=azure_connector_surf.client\n",
    "        )\n",
    "        graph = MA.initialize_graph()\n",
    "        llm_output = MA.generate_multi_agent_answer(llm_input, graph)\n",
    "    # running in single-agent mode\n",
    "    elif os.getenv(\"MODE\") == \"single-agent\":\n",
    "        if llm_input.get(\"new_message\") and check_relevance(\n",
    "            azure_connector_surf, llm_input.get(\"new_message\")\n",
    "        ):\n",
    "            llm_output[\"response\"] = (\n",
    "                \"I'm here to help with trading and financial market queries. If you think your ask relates to trading and isn't addressed, please report a bug using the bottom right panel.\"\n",
    "            )\n",
    "            return llm_output\n",
    "        SA = Single_Agent(azure_connector_surf.client)\n",
    "        results_string, results_json, function_name = SA.generate_answer(\n",
    "            llm_input=llm_input, content=content\n",
    "        )\n",
    "        llm_output[\"response\"] = results_string\n",
    "        llm_output[\"chart_info\"] = results_json\n",
    "        llm_output[\"function_call\"] = function_name\n",
    "\n",
    "    if not DEBUG:\n",
    "        llm_output[\"file\"] = fileProcessor.text_to_speech(llm_output[\"response\"])\n",
    "\n",
    "    return llm_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to update token costs. Using static costs.\n",
      "/home/sepehr/anaconda3/envs/sepehr/lib/python3.10/site-packages/tokencost/constants.py:61: RuntimeWarning: coroutine 'update_token_costs' was never awaited\n",
      "  TOKEN_COSTS = TOKEN_COSTS_STATIC\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n",
      "WARNING:root:gpt-3.5-turbo-0613 may update over time. Returning num tokens assuming gpt-3.5-turbo-0613-0613.\n",
      "WARNING:root:gpt-3.5-turbo-0613 may update over time. Returning num tokens assuming gpt-3.5-turbo-0613-0613.\n",
      "WARNING:root:gpt-3.5-turbo-0613 may update over time. Returning num tokens assuming gpt-3.5-turbo-0613-0613.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the trend of NQ and ES stock from May-1-2024 12:00:00 until May-5-2024 12:00:00?    =>    {'response': 'The trend of the NQ and ES stocks from May-1-2024 12:00:00 until May-5-2024 12:00:00 is a mild bullish (upward) trend.', 'chart_info': {}}\n",
      "Inference Time: 42.68517088890076 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:gpt-3.5-turbo-0613 may update over time. Returning num tokens assuming gpt-3.5-turbo-0613-0613.\n",
      "WARNING:root:gpt-3.5-turbo-0613 may update over time. Returning num tokens assuming gpt-3.5-turbo-0613-0613.\n",
      "WARNING:root:gpt-3.5-turbo-0613 may update over time. Returning num tokens assuming gpt-3.5-turbo-0613-0613.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "response: ([38778.0, 38901.0, 39259.0, 39094.0, 39117.0, 39168.0, 39118.0, 39476.0, 39481.0, 39687.0, 39502.0, 39527.0, 39613.0, 39619.0, 39558.0], [Timestamp('2024-06-20 06:30:00'), Timestamp('2024-06-20 08:00:00'), Timestamp('2024-06-20 11:30:00'), Timestamp('2024-06-20 12:50:00'), Timestamp('2024-06-20 18:00:00'), Timestamp('2024-06-20 22:30:00'), Timestamp('2024-06-21 00:00:00'), Timestamp('2024-06-21 05:20:00'), Timestamp('2024-06-21 06:40:00'), Timestamp('2024-06-21 07:20:00'), Timestamp('2024-06-21 08:40:00'), Timestamp('2024-06-21 10:30:00'), Timestamp('2024-06-21 11:20:00'), Timestamp('2024-06-21 12:50:00'), Timestamp('2024-06-21 13:00:00')], [Timestamp('2024-06-20 07:20:00'), Timestamp('2024-06-20 08:50:00'), Timestamp('2024-06-20 12:20:00'), Timestamp('2024-06-20 13:40:00'), Timestamp('2024-06-20 20:00:00'), Timestamp('2024-06-20 23:30:00'), Timestamp('2024-06-21 01:40:00'), Timestamp('2024-06-21 06:10:00'), Timestamp('2024-06-21 07:30:00'), Timestamp('2024-06-21 08:10:00'), Timestamp('2024-06-21 09:30:00'), Timestamp('2024-06-21 11:20:00'), Timestamp('2024-06-21 12:10:00'), Timestamp('2024-06-21 13:40:00'), Timestamp('2024-06-21 13:50:00')], [Timestamp('2024-06-21 13:58:00'), Timestamp('2024-06-21 13:58:00'), Timestamp('2024-06-21 13:58:00'), Timestamp('2024-06-21 13:58:00'), Timestamp('2024-06-21 13:58:00'), Timestamp('2024-06-21 13:58:00'), Timestamp('2024-06-21 13:58:00'), Timestamp('2024-06-21 13:58:00'), Timestamp('2024-06-21 13:58:00'), Timestamp('2024-06-21 13:58:00'), Timestamp('2024-06-21 13:58:00'), Timestamp('2024-06-21 13:58:00'), Timestamp('2024-06-21 13:58:00'), Timestamp('2024-06-21 13:58:00'), Timestamp('2024-06-21 13:58:00')], [6.1, 2.1, 3.5, 3.0, 53.6, 8.0, 41.4, 10.7, 6.6, 3.0, 2.7, 3.9, 3.2, 3.3, 2.6], 'The support and resistance levels for YM based on historical price data with a lookback period of 5 days and a timeframe of 10min are as follows:\\n\\n- Levels: [38778.0, 38901.0, 39259.0, 39094.0, 39117.0, 39168.0, 39118.0, 39476.0, 39481.0, 39687.0, 39502.0, 39527.0, 39613.0, 39619.0, 39558.0]\\n\\nThese levels are determined based on historical price data and indicate areas where the price is likely to encounter support or resistance. The associated scores indicate the strength or significance of each level, with higher scores indicating stronger levels.')\n",
      "\n",
      "Calculate Support and Resistance Levels of YM and NQ by looking back up to past 5 days and timeframe of 10 minutes.    =>    {'response': 'The support and resistance levels for YM based on historical price data with a lookback period of 5 days and a timeframe of 10min are as follows:\\n\\n- Levels: [38778.0, 38901.0, 39259.0, 39094.0, 39117.0, 39168.0, 39118.0, 39476.0, 39481.0, 39687.0, 39502.0, 39527.0, 39613.0, 39619.0, 39558.0]\\n\\nThese levels are determined based on historical price data and indicate areas where the price is likely to encounter support or resistance. The associated scores indicate the strength or significance of each level, with higher scores indicating stronger levels.', 'chart_info': {'levels_prices': [38778.0, 38901.0, 39259.0, 39094.0, 39117.0, 39168.0, 39118.0, 39476.0, 39481.0, 39687.0, 39502.0, 39527.0, 39613.0, 39619.0, 39558.0], 'levels_start_timestamps': [Timestamp('2024-06-20 06:30:00'), Timestamp('2024-06-20 08:00:00'), Timestamp('2024-06-20 11:30:00'), Timestamp('2024-06-20 12:50:00'), Timestamp('2024-06-20 18:00:00'), Timestamp('2024-06-20 22:30:00'), Timestamp('2024-06-21 00:00:00'), Timestamp('2024-06-21 05:20:00'), Timestamp('2024-06-21 06:40:00'), Timestamp('2024-06-21 07:20:00'), Timestamp('2024-06-21 08:40:00'), Timestamp('2024-06-21 10:30:00'), Timestamp('2024-06-21 11:20:00'), Timestamp('2024-06-21 12:50:00'), Timestamp('2024-06-21 13:00:00')], 'levels_detect_timestamps': [Timestamp('2024-06-20 07:20:00'), Timestamp('2024-06-20 08:50:00'), Timestamp('2024-06-20 12:20:00'), Timestamp('2024-06-20 13:40:00'), Timestamp('2024-06-20 20:00:00'), Timestamp('2024-06-20 23:30:00'), Timestamp('2024-06-21 01:40:00'), Timestamp('2024-06-21 06:10:00'), Timestamp('2024-06-21 07:30:00'), Timestamp('2024-06-21 08:10:00'), Timestamp('2024-06-21 09:30:00'), Timestamp('2024-06-21 11:20:00'), Timestamp('2024-06-21 12:10:00'), Timestamp('2024-06-21 13:40:00'), Timestamp('2024-06-21 13:50:00')], 'levels_end_timestamps': [Timestamp('2024-06-21 13:58:00'), Timestamp('2024-06-21 13:58:00'), Timestamp('2024-06-21 13:58:00'), Timestamp('2024-06-21 13:58:00'), Timestamp('2024-06-21 13:58:00'), Timestamp('2024-06-21 13:58:00'), Timestamp('2024-06-21 13:58:00'), Timestamp('2024-06-21 13:58:00'), Timestamp('2024-06-21 13:58:00'), Timestamp('2024-06-21 13:58:00'), Timestamp('2024-06-21 13:58:00'), Timestamp('2024-06-21 13:58:00'), Timestamp('2024-06-21 13:58:00'), Timestamp('2024-06-21 13:58:00'), Timestamp('2024-06-21 13:58:00')], 'levels_scores': [6.1, 2.1, 3.5, 3.0, 53.6, 8.0, 41.4, 10.7, 6.6, 3.0, 2.7, 3.9, 3.2, 3.3, 2.6], 'function_name': 'calculate_sr', 'symbol': 'YM', 'timeframe': '10min', 'response': 'The support and resistance levels for YM based on historical price data with a lookback period of 5 days and a timeframe of 10min are as follows:\\n\\n- Levels: [38778.0, 38901.0, 39259.0, 39094.0, 39117.0, 39168.0, 39118.0, 39476.0, 39481.0, 39687.0, 39502.0, 39527.0, 39613.0, 39619.0, 39558.0]\\n\\nThese levels are determined based on historical price data and indicate areas where the price is likely to encounter support or resistance. The associated scores indicate the strength or significance of each level, with higher scores indicating stronger levels.'}}\n",
      "Inference Time: 19.188952684402466 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:gpt-3.5-turbo-0613 may update over time. Returning num tokens assuming gpt-3.5-turbo-0613-0613.\n",
      "WARNING:root:gpt-3.5-turbo-0613 may update over time. Returning num tokens assuming gpt-3.5-turbo-0613-0613.\n",
      "WARNING:root:gpt-3.5-turbo-0613 may update over time. Returning num tokens assuming gpt-3.5-turbo-0613-0613.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What would be the stop loss of trading short positinos based on NQ and ES and minmax method by looking back up to 30 candles?    =>    {'response': 'The stop loss for trading short positions based on NQ and ES using the minmax method and looking back up to 30 candles would be 20006.75. The risk on the trade is 0.5 points. This stop loss is calculated based on the maximum high price of the previous 30 candles.', 'chart_info': {'stop_loss': {'sl': [20006.75], 'risk': [0.5], 'info': ['calculated based on maximum high price of previous 30 candles']}, 'symbol': 'NQ', 'timeframe': '10min'}}\n",
      "Inference Time: 15.910284757614136 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:gpt-3.5-turbo-0613 may update over time. Returning num tokens assuming gpt-3.5-turbo-0613-0613.\n",
      "WARNING:root:gpt-3.5-turbo-0613 may update over time. Returning num tokens assuming gpt-3.5-turbo-0613-0613.\n",
      "WARNING:root:gpt-3.5-turbo-0613 may update over time. Returning num tokens assuming gpt-3.5-turbo-0613-0613.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How much would be the take-profit of the NQ with the stop loss of 10 and direction of 1?    =>    {'response': 'Based on the provided information, the take-profit (TP) for the NQ with a stop loss of 10 and a direction of 1 cannot be determined. The calculation did not return any specific take-profit levels.', 'chart_info': {'take_profit': {'tp': [], 'info': []}, 'symbol': 'NQ', 'timeframe': '10min'}}\n",
      "Inference Time: 27.161231756210327 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:gpt-3.5-turbo-0613 may update over time. Returning num tokens assuming gpt-3.5-turbo-0613-0613.\n",
      "WARNING:root:gpt-3.5-turbo-0613 may update over time. Returning num tokens assuming gpt-3.5-turbo-0613-0613.\n",
      "WARNING:root:gpt-3.5-turbo-0613 may update over time. Returning num tokens assuming gpt-3.5-turbo-0613-0613.\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for FunctionMessage\nname\n  field required (type=value_error.missing)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m input_json[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnew_message\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m prompt\n\u001b[1;32m     25\u001b[0m start \u001b[38;5;241m=\u001b[39m time()\n\u001b[0;32m---> 26\u001b[0m output_json \u001b[38;5;241m=\u001b[39m \u001b[43mllm_surf\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_json\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m end \u001b[38;5;241m=\u001b[39m time()\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m    =>    \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_json\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Work/TenSurf/TenSurf_Brain/main.py:183\u001b[0m, in \u001b[0;36mllm_surf\u001b[0;34m(llm_input)\u001b[0m\n\u001b[1;32m    179\u001b[0m     MA \u001b[38;5;241m=\u001b[39m Multi_Agent(\n\u001b[1;32m    180\u001b[0m         ChatWithOpenai\u001b[38;5;241m=\u001b[39mChatWithOpenai, client\u001b[38;5;241m=\u001b[39mazure_connector_surf\u001b[38;5;241m.\u001b[39mclient\n\u001b[1;32m    181\u001b[0m     )\n\u001b[1;32m    182\u001b[0m     graph \u001b[38;5;241m=\u001b[39m MA\u001b[38;5;241m.\u001b[39minitialize_graph()\n\u001b[0;32m--> 183\u001b[0m     llm_output \u001b[38;5;241m=\u001b[39m \u001b[43mMA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_multi_agent_answer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;66;03m# running in single-agent mode\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMODE\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msingle-agent\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/Work/TenSurf/TenSurf_Brain/multi_agent/multi_agent.py:127\u001b[0m, in \u001b[0;36mMulti_Agent.generate_multi_agent_answer\u001b[0;34m(self, input_json, graph)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_multi_agent_answer\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_json, graph):\n\u001b[1;32m    114\u001b[0m     generated_messages \u001b[38;5;241m=\u001b[39m graph\u001b[38;5;241m.\u001b[39mstream(\n\u001b[1;32m    115\u001b[0m         {\n\u001b[1;32m    116\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    124\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecursion_limit\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m150\u001b[39m},\n\u001b[1;32m    125\u001b[0m     )\n\u001b[0;32m--> 127\u001b[0m     generated_messages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgenerated_messages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m     k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(generated_messages[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mkeys())[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_json\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m generated_messages[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][k]:\n",
      "File \u001b[0;32m~/anaconda3/envs/sepehr/lib/python3.10/site-packages/langgraph/pregel/__init__.py:963\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, stream_mode, output_keys, input_keys, interrupt_before, interrupt_after, debug)\u001b[0m\n\u001b[1;32m    960\u001b[0m         \u001b[38;5;28;01mdel\u001b[39;00m fut, task\n\u001b[1;32m    962\u001b[0m \u001b[38;5;66;03m# panic on failure or timeout\u001b[39;00m\n\u001b[0;32m--> 963\u001b[0m \u001b[43m_panic_or_proceed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minflight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[38;5;66;03m# don't keep futures around in memory longer than needed\u001b[39;00m\n\u001b[1;32m    965\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m done, inflight, futures\n",
      "File \u001b[0;32m~/anaconda3/envs/sepehr/lib/python3.10/site-packages/langgraph/pregel/__init__.py:1489\u001b[0m, in \u001b[0;36m_panic_or_proceed\u001b[0;34m(done, inflight, step)\u001b[0m\n\u001b[1;32m   1487\u001b[0m             inflight\u001b[38;5;241m.\u001b[39mpop()\u001b[38;5;241m.\u001b[39mcancel()\n\u001b[1;32m   1488\u001b[0m         \u001b[38;5;66;03m# raise the exception\u001b[39;00m\n\u001b[0;32m-> 1489\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m   1491\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inflight:\n\u001b[1;32m   1492\u001b[0m     \u001b[38;5;66;03m# if we got here means we timed out\u001b[39;00m\n\u001b[1;32m   1493\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m inflight:\n\u001b[1;32m   1494\u001b[0m         \u001b[38;5;66;03m# cancel all pending tasks\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/sepehr/lib/python3.10/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m~/anaconda3/envs/sepehr/lib/python3.10/site-packages/langgraph/pregel/retry.py:66\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy)\u001b[0m\n\u001b[1;32m     64\u001b[0m task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# if successful, end\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/sepehr/lib/python3.10/site-packages/langchain_core/runnables/base.py:2502\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   2498\u001b[0m config \u001b[38;5;241m=\u001b[39m patch_config(\n\u001b[1;32m   2499\u001b[0m     config, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq:step:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2500\u001b[0m )\n\u001b[1;32m   2501\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2502\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2503\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2504\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m~/anaconda3/envs/sepehr/lib/python3.10/site-packages/langgraph/utils.py:95\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m accepts_config(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc):\n\u001b[1;32m     94\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config\n\u001b[0;32m---> 95\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse:\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m~/Work/TenSurf/TenSurf_Brain/multi_agent/utils.py:180\u001b[0m, in \u001b[0;36mUtils.tool_node\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    176\u001b[0m     tool_input \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(\n\u001b[1;32m    177\u001b[0m         last_message\u001b[38;5;241m.\u001b[39madditional_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marguments\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    178\u001b[0m     )\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 180\u001b[0m     function_message \u001b[38;5;241m=\u001b[39m \u001b[43mFunctionMessage\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse: \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlast_message\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: [function_message], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_json\u001b[39m\u001b[38;5;124m\"\u001b[39m:output_json}\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tool_input) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__arg1\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m tool_input:\n",
      "File \u001b[0;32m~/anaconda3/envs/sepehr/lib/python3.10/site-packages/langchain_core/messages/base.py:47\u001b[0m, in \u001b[0;36mBaseMessage.__init__\u001b[0;34m(self, content, **kwargs)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28mself\u001b[39m, content: Union[\u001b[38;5;28mstr\u001b[39m, List[Union[\u001b[38;5;28mstr\u001b[39m, Dict]]], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any\n\u001b[1;32m     45\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Pass in content as positional arg.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/sepehr/lib/python3.10/site-packages/pydantic/v1/main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[0;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for FunctionMessage\nname\n  field required (type=value_error.missing)"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "import input_filter\n",
    "from main import llm_surf\n",
    "\n",
    "# sample prompt for each function\n",
    "prompts = [\n",
    "    # detect_trend\n",
    "    \"What is the trend of NQ and ES stock from May-1-2024 12:00:00 until May-5-2024 12:00:00?\",\n",
    "    # calculate_sr\n",
    "    \"Calculate Support and Resistance Levels of YM and NQ by looking back up to past 5 days and timeframe of 10 minutes.\",\n",
    "    # calculate_sl\n",
    "    \"What would be the stop loss of trading short positinos based on NQ and ES and minmax method by looking back up to 30 candles?\",\n",
    "    # calculate_tp\n",
    "    \"How much would be the take-profit of the NQ with the stop loss of 10 and direction of 1?\",\n",
    "    # bias_detection\n",
    "    \"Tell me about the bias of ES on the market.\",\n",
    "\t# irrelevant\n",
    "\t\"What are the rules of basketball?\"\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    input_json = input_filter.front_end_json_sample\n",
    "    input_json[\"new_message\"] = prompt\n",
    "    start = time()\n",
    "    output_json = llm_surf(input_json)\n",
    "    end = time()\n",
    "    print(f\"{prompt}    =>    {output_json}\")\n",
    "    print(f\"Inference Time: {end - start} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sepehr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
