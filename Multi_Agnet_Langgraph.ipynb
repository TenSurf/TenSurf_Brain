{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VkDMcuAkyfpD"
      },
      "outputs": [],
      "source": [
        "# from langchain.callbacks.base import BaseCallbackHandler\n",
        "\n",
        "# class CustomCallbackHandler(BaseCallbackHandler):\n",
        "#     def on_tool_start(self, tool_name: str, **kwargs):\n",
        "#         print(f\"Tool {tool_name} started with arguments: {kwargs}\")\n",
        "\n",
        "#     def on_tool_end(self, tool_name: str, output: str, **kwargs):\n",
        "#         print(f\"Tool {tool_name} finished with output: {output}\")\n",
        "\n",
        "#     def on_llm_start(self, **kwargs):\n",
        "#         print(\"LLM started\")\n",
        "\n",
        "#     def on_llm_end(self, output: str, **kwargs):\n",
        "#         print(f\"LLM finished with output: {output}\")\n",
        "\n",
        "# # Example usage\n",
        "# handler = CustomCallbackHandler()\n",
        "\n",
        "# # Simulating some events\n",
        "# handler.on_tool_start(\"example_tool\", arg1=\"value1\", arg2=\"value2\")\n",
        "# handler.on_tool_end(\"example_tool\", output=\"tool output\")\n",
        "# handler.on_llm_start()\n",
        "# handler.on_llm_end(output=\"LLM output\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfknVBncqyyR"
      },
      "source": [
        "# Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XvP4kQpiU3bz",
        "outputId": "ee8b36e8-91b4-4e1c-c5d1-c5da3c0e45d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting Openai\n",
            "  Downloading openai-1.30.3-py3-none-any.whl (320 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/320.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.6/320.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.6/320.6 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from Openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from Openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from Openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from Openai) (2.7.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from Openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from Openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from Openai) (4.11.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->Openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->Openai) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->Openai) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->Openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->Openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->Openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->Openai) (2.18.2)\n",
            "Installing collected packages: h11, httpcore, httpx, Openai\n",
            "Successfully installed Openai-1.30.3 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0\n"
          ]
        }
      ],
      "source": [
        "!pip install Openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CcrPkYWKEnFJ",
        "outputId": "5ca313f0-57f9-4535-eefb-218849064323"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libgail-common libgail18 libgtk2.0-0 libgtk2.0-bin libgtk2.0-common libgvc6-plugins-gtk\n",
            "  librsvg2-common libxdot4\n",
            "Suggested packages:\n",
            "  gvfs\n",
            "The following NEW packages will be installed:\n",
            "  libgail-common libgail18 libgraphviz-dev libgtk2.0-0 libgtk2.0-bin libgtk2.0-common\n",
            "  libgvc6-plugins-gtk librsvg2-common libxdot4\n",
            "0 upgraded, 9 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 2,433 kB of archives.\n",
            "After this operation, 7,694 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgtk2.0-common all 2.24.33-2ubuntu2 [125 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgtk2.0-0 amd64 2.24.33-2ubuntu2 [2,037 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgail18 amd64 2.24.33-2ubuntu2 [15.9 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgail-common amd64 2.24.33-2ubuntu2 [132 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libxdot4 amd64 2.42.2-6 [16.4 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libgvc6-plugins-gtk amd64 2.42.2-6 [22.6 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libgraphviz-dev amd64 2.42.2-6 [58.5 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgtk2.0-bin amd64 2.24.33-2ubuntu2 [7,932 B]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 librsvg2-common amd64 2.52.5+dfsg-3ubuntu0.2 [17.7 kB]\n",
            "Fetched 2,433 kB in 0s (6,208 kB/s)\n",
            "Selecting previously unselected package libgtk2.0-common.\n",
            "(Reading database ... 121918 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libgtk2.0-common_2.24.33-2ubuntu2_all.deb ...\n",
            "Unpacking libgtk2.0-common (2.24.33-2ubuntu2) ...\n",
            "Selecting previously unselected package libgtk2.0-0:amd64.\n",
            "Preparing to unpack .../1-libgtk2.0-0_2.24.33-2ubuntu2_amd64.deb ...\n",
            "Unpacking libgtk2.0-0:amd64 (2.24.33-2ubuntu2) ...\n",
            "Selecting previously unselected package libgail18:amd64.\n",
            "Preparing to unpack .../2-libgail18_2.24.33-2ubuntu2_amd64.deb ...\n",
            "Unpacking libgail18:amd64 (2.24.33-2ubuntu2) ...\n",
            "Selecting previously unselected package libgail-common:amd64.\n",
            "Preparing to unpack .../3-libgail-common_2.24.33-2ubuntu2_amd64.deb ...\n",
            "Unpacking libgail-common:amd64 (2.24.33-2ubuntu2) ...\n",
            "Selecting previously unselected package libxdot4:amd64.\n",
            "Preparing to unpack .../4-libxdot4_2.42.2-6_amd64.deb ...\n",
            "Unpacking libxdot4:amd64 (2.42.2-6) ...\n",
            "Selecting previously unselected package libgvc6-plugins-gtk.\n",
            "Preparing to unpack .../5-libgvc6-plugins-gtk_2.42.2-6_amd64.deb ...\n",
            "Unpacking libgvc6-plugins-gtk (2.42.2-6) ...\n",
            "Selecting previously unselected package libgraphviz-dev:amd64.\n",
            "Preparing to unpack .../6-libgraphviz-dev_2.42.2-6_amd64.deb ...\n",
            "Unpacking libgraphviz-dev:amd64 (2.42.2-6) ...\n",
            "Selecting previously unselected package libgtk2.0-bin.\n",
            "Preparing to unpack .../7-libgtk2.0-bin_2.24.33-2ubuntu2_amd64.deb ...\n",
            "Unpacking libgtk2.0-bin (2.24.33-2ubuntu2) ...\n",
            "Selecting previously unselected package librsvg2-common:amd64.\n",
            "Preparing to unpack .../8-librsvg2-common_2.52.5+dfsg-3ubuntu0.2_amd64.deb ...\n",
            "Unpacking librsvg2-common:amd64 (2.52.5+dfsg-3ubuntu0.2) ...\n",
            "Setting up libxdot4:amd64 (2.42.2-6) ...\n",
            "Setting up librsvg2-common:amd64 (2.52.5+dfsg-3ubuntu0.2) ...\n",
            "Setting up libgtk2.0-common (2.24.33-2ubuntu2) ...\n",
            "Setting up libgtk2.0-0:amd64 (2.24.33-2ubuntu2) ...\n",
            "Setting up libgvc6-plugins-gtk (2.42.2-6) ...\n",
            "Setting up libgail18:amd64 (2.24.33-2ubuntu2) ...\n",
            "Setting up libgtk2.0-bin (2.24.33-2ubuntu2) ...\n",
            "Setting up libgail-common:amd64 (2.24.33-2ubuntu2) ...\n",
            "Setting up libgraphviz-dev:amd64 (2.42.2-6) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libgdk-pixbuf-2.0-0:amd64 (2.42.8+dfsg-1ubuntu0.2) ...\n",
            "Collecting pygraphviz\n",
            "  Downloading pygraphviz-1.13.tar.gz (104 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.6/104.6 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: pygraphviz\n",
            "  Building wheel for pygraphviz (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pygraphviz: filename=pygraphviz-1.13-cp310-cp310-linux_x86_64.whl size=168496 sha256=646a2437e5cc22400331c8348f121910800efc9a8fa9b5ccea79d9449eedf280\n",
            "  Stored in directory: /root/.cache/pip/wheels/c5/96/10/6c25add1fffc368b1927252bf73b63fcb938de8f4486e23691\n",
            "Successfully built pygraphviz\n",
            "Installing collected packages: pygraphviz\n",
            "Successfully installed pygraphviz-1.13\n"
          ]
        }
      ],
      "source": [
        "!apt install libgraphviz-dev\n",
        "!pip install pygraphviz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5V8z7XbSFNAh",
        "outputId": "6fca4cde-7c14-4fc6-ee1a-d6a9531359ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting zigzag\n",
            "  Downloading zigzag-0.3.2.tar.gz (112 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/112.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.3/112.3 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting Cython<0.30,>=0.29 (from zigzag)\n",
            "  Downloading Cython-0.29.37-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from zigzag) (1.25.2)\n",
            "Building wheels for collected packages: zigzag\n",
            "  Building wheel for zigzag (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for zigzag: filename=zigzag-0.3.2-cp310-cp310-manylinux_2_35_x86_64.whl size=782650 sha256=f8528e56b9a27249aa97d98a580dc8cf74c3cebe318bd52436295076dc8613f9\n",
            "  Stored in directory: /root/.cache/pip/wheels/e2/27/4f/a2cca46b8db81b9fdbec9d8cdd22ad52ccfce1deb318c108c4\n",
            "Successfully built zigzag\n",
            "Installing collected packages: Cython, zigzag\n",
            "  Attempting uninstall: Cython\n",
            "    Found existing installation: Cython 3.0.10\n",
            "    Uninstalling Cython-3.0.10:\n",
            "      Successfully uninstalled Cython-3.0.10\n",
            "Successfully installed Cython-0.29.37 zigzag-0.3.2\n"
          ]
        }
      ],
      "source": [
        "!pip install zigzag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hnLtdz5cFwdn",
        "outputId": "664d8643-cd1f-465b-fba5-d996cbc3be5e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting influxdb_client\n",
            "  Downloading influxdb_client-1.43.0-py3-none-any.whl (744 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m744.7/744.7 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting reactivex>=4.0.4 (from influxdb_client)\n",
            "  Downloading reactivex-4.0.4-py3-none-any.whl (217 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m217.8/217.8 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.10/dist-packages (from influxdb_client) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from influxdb_client) (2.8.2)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.10/dist-packages (from influxdb_client) (67.7.2)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from influxdb_client) (2.0.7)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.5.3->influxdb_client) (1.16.0)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from reactivex>=4.0.4->influxdb_client) (4.11.0)\n",
            "Installing collected packages: reactivex, influxdb_client\n",
            "Successfully installed influxdb_client-1.43.0 reactivex-4.0.4\n"
          ]
        }
      ],
      "source": [
        "!pip install influxdb_client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eiiOvTlEm1vz",
        "outputId": "ef19ed85-d9c6-4a7c-9530-fb1219332b96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting import_ipynb\n",
            "  Downloading import_ipynb-0.1.4-py3-none-any.whl (4.1 kB)\n",
            "Requirement already satisfied: IPython in /usr/local/lib/python3.10/dist-packages (from import_ipynb) (7.34.0)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (from import_ipynb) (5.10.4)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from IPython->import_ipynb) (67.7.2)\n",
            "Collecting jedi>=0.16 (from IPython->import_ipynb)\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from IPython->import_ipynb) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from IPython->import_ipynb) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from IPython->import_ipynb) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from IPython->import_ipynb) (3.0.43)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from IPython->import_ipynb) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from IPython->import_ipynb) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from IPython->import_ipynb) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from IPython->import_ipynb) (4.9.0)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.10/dist-packages (from nbformat->import_ipynb) (2.19.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat->import_ipynb) (4.19.2)\n",
            "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.10/dist-packages (from nbformat->import_ipynb) (5.7.2)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->IPython->import_ipynb) (0.8.4)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->import_ipynb) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->import_ipynb) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->import_ipynb) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->import_ipynb) (0.18.1)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core!=5.0.*,>=4.12->nbformat->import_ipynb) (4.2.2)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->IPython->import_ipynb) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->IPython->import_ipynb) (0.2.13)\n",
            "Installing collected packages: jedi, import_ipynb\n",
            "Successfully installed import_ipynb-0.1.4 jedi-0.19.1\n"
          ]
        }
      ],
      "source": [
        "!pip install import_ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZo2S6-jmtdz",
        "outputId": "d23e3cba-be7b-4420-8ff4-0a1f3a7702d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting crewai\n",
            "  Downloading crewai-0.30.11-py3-none-any.whl (66 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/66.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m61.4/66.1 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.1/66.1 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting appdirs<2.0.0,>=1.4.4 (from crewai)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: click<9.0.0,>=8.1.7 in /usr/local/lib/python3.10/dist-packages (from crewai) (8.1.7)\n",
            "Collecting embedchain<0.2.0,>=0.1.98 (from crewai)\n",
            "  Downloading embedchain-0.1.104-py3-none-any.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting instructor<0.6.0,>=0.5.2 (from crewai)\n",
            "  Downloading instructor-0.5.2-py3-none-any.whl (33 kB)\n",
            "Collecting langchain<0.2.0,>=0.1.10 (from crewai)\n",
            "  Downloading langchain-0.1.20-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: openai<2.0.0,>=1.13.3 in /usr/local/lib/python3.10/dist-packages (from crewai) (1.30.3)\n",
            "Collecting opentelemetry-api<2.0.0,>=1.22.0 (from crewai)\n",
            "  Downloading opentelemetry_api-1.24.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.1/60.1 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.22.0 (from crewai)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_http-1.24.0-py3-none-any.whl (16 kB)\n",
            "Collecting opentelemetry-sdk<2.0.0,>=1.22.0 (from crewai)\n",
            "  Downloading opentelemetry_sdk-1.24.0-py3-none-any.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.1/106.1 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3.0.0,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from crewai) (2.7.1)\n",
            "Collecting python-dotenv<2.0.0,>=1.0.0 (from crewai)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: regex<2024.0.0,>=2023.12.25 in /usr/local/lib/python3.10/dist-packages (from crewai) (2023.12.25)\n",
            "Collecting alembic<2.0.0,>=1.13.1 (from embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading alembic-1.13.1-py3-none-any.whl (233 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.4/233.4 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4<5.0.0,>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from embedchain<0.2.0,>=0.1.98->crewai) (4.12.3)\n",
            "Collecting chromadb<0.6.0,>=0.5.0 (from embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading chromadb-0.5.0-py3-none-any.whl (526 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m526.8/526.8 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: google-cloud-aiplatform<2.0.0,>=1.26.1 in /usr/local/lib/python3.10/dist-packages (from embedchain<0.2.0,>=0.1.98->crewai) (1.52.0)\n",
            "Collecting gptcache<0.2.0,>=0.1.43 (from embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading gptcache-0.1.43-py3-none-any.whl (131 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.5/131.5 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-cohere<0.2.0,>=0.1.4 (from embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading langchain_cohere-0.1.5-py3-none-any.whl (30 kB)\n",
            "Collecting langchain-openai<0.2.0,>=0.1.7 (from embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading langchain_openai-0.1.7-py3-none-any.whl (34 kB)\n",
            "Collecting posthog<4.0.0,>=3.0.2 (from embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading posthog-3.5.0-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pypdf<5.0.0,>=4.0.1 (from embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading pypdf-4.2.0-py3-none-any.whl (290 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.4/290.4 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pysbd<0.4.0,>=0.3.4 (from embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading pysbd-0.3.4-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: rich<14.0.0,>=13.7.0 in /usr/local/lib/python3.10/dist-packages (from embedchain<0.2.0,>=0.1.98->crewai) (13.7.1)\n",
            "Collecting schema<0.8.0,>=0.7.5 (from embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading schema-0.7.7-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: sqlalchemy<3.0.0,>=2.0.27 in /usr/local/lib/python3.10/dist-packages (from embedchain<0.2.0,>=0.1.98->crewai) (2.0.30)\n",
            "Collecting tiktoken<0.8.0,>=0.7.0 (from embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp<4.0.0,>=3.9.1 in /usr/local/lib/python3.10/dist-packages (from instructor<0.6.0,>=0.5.2->crewai) (3.9.5)\n",
            "Collecting docstring-parser<0.16,>=0.15 (from instructor<0.6.0,>=0.5.2->crewai)\n",
            "  Downloading docstring_parser-0.15-py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.2.3 in /usr/local/lib/python3.10/dist-packages (from instructor<0.6.0,>=0.5.2->crewai) (8.3.0)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from instructor<0.6.0,>=0.5.2->crewai) (0.9.4)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain<0.2.0,>=0.1.10->crewai) (6.0.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.2.0,>=0.1.10->crewai) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain<0.2.0,>=0.1.10->crewai)\n",
            "  Downloading dataclasses_json-0.6.6-py3-none-any.whl (28 kB)\n",
            "Collecting langchain-community<0.1,>=0.0.38 (from langchain<0.2.0,>=0.1.10->crewai)\n",
            "  Downloading langchain_community-0.0.38-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core<0.2.0,>=0.1.52 (from langchain<0.2.0,>=0.1.10->crewai)\n",
            "  Downloading langchain_core-0.1.52-py3-none-any.whl (302 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.9/302.9 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-text-splitters<0.1,>=0.0.1 (from langchain<0.2.0,>=0.1.10->crewai)\n",
            "  Downloading langchain_text_splitters-0.0.2-py3-none-any.whl (23 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain<0.2.0,>=0.1.10->crewai)\n",
            "  Downloading langsmith-0.1.63-py3-none-any.whl (122 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.8/122.8 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain<0.2.0,>=0.1.10->crewai) (1.25.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain<0.2.0,>=0.1.10->crewai) (2.31.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.13.3->crewai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.13.3->crewai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.13.3->crewai) (0.27.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.13.3->crewai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.13.3->crewai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.13.3->crewai) (4.11.0)\n",
            "Collecting deprecated>=1.2.6 (from opentelemetry-api<2.0.0,>=1.22.0->crewai)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting importlib-metadata<=7.0,>=6.0 (from opentelemetry-api<2.0.0,>=1.22.0->crewai)\n",
            "  Downloading importlib_metadata-7.0.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.22.0->crewai) (1.63.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.24.0 (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.22.0->crewai)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.24.0-py3-none-any.whl (17 kB)\n",
            "Collecting opentelemetry-proto==1.24.0 (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.22.0->crewai)\n",
            "  Downloading opentelemetry_proto-1.24.0-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf<5.0,>=3.19 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-proto==1.24.0->opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.22.0->crewai) (3.20.3)\n",
            "Collecting opentelemetry-semantic-conventions==0.45b0 (from opentelemetry-sdk<2.0.0,>=1.22.0->crewai)\n",
            "  Downloading opentelemetry_semantic_conventions-0.45b0-py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.4.2->crewai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.4.2->crewai) (2.18.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor<0.6.0,>=0.5.2->crewai) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor<0.6.0,>=0.5.2->crewai) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor<0.6.0,>=0.5.2->crewai) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor<0.6.0,>=0.5.2->crewai) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor<0.6.0,>=0.5.2->crewai) (1.9.4)\n",
            "Collecting Mako (from alembic<2.0.0,>=1.13.1->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading Mako-1.3.5-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.13.3->crewai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.13.3->crewai) (1.2.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.12.2->embedchain<0.2.0,>=0.1.98->crewai) (2.5)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.6.0,>=0.5.0->embedchain<0.2.0,>=0.1.98->crewai) (1.2.1)\n",
            "Collecting chroma-hnswlib==0.7.3 (from chromadb<0.6.0,>=0.5.0->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading chroma_hnswlib-0.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi>=0.95.2 (from chromadb<0.6.0,>=0.5.0->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading fastapi-0.111.0-py3-none-any.whl (91 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvicorn[standard]>=0.18.3 (from chromadb<0.6.0,>=0.5.0->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading uvicorn-0.29.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnxruntime>=1.14.1 (from chromadb<0.6.0,>=0.5.0->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading onnxruntime-1.18.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb<0.6.0,>=0.5.0->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.24.0-py3-none-any.whl (18 kB)\n",
            "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb<0.6.0,>=0.5.0->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading opentelemetry_instrumentation_fastapi-0.45b0-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.6.0,>=0.5.0->embedchain<0.2.0,>=0.1.98->crewai) (0.19.1)\n",
            "Collecting pypika>=0.48.9 (from chromadb<0.6.0,>=0.5.0->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting overrides>=7.3.1 (from chromadb<0.6.0,>=0.5.0->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb<0.6.0,>=0.5.0->embedchain<0.2.0,>=0.1.98->crewai) (6.4.0)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.6.0,>=0.5.0->embedchain<0.2.0,>=0.1.98->crewai) (1.64.0)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb<0.6.0,>=0.5.0->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading bcrypt-4.1.3-cp39-abi3-manylinux_2_28_x86_64.whl (283 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m283.7/283.7 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting kubernetes>=28.1.0 (from chromadb<0.6.0,>=0.5.0->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading kubernetes-29.0.0-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mmh3>=4.0.1 (from chromadb<0.6.0,>=0.5.0->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading mmh3-4.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting orjson>=3.9.12 (from chromadb<0.6.0,>=0.5.0->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading orjson-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain<0.2.0,>=0.1.10->crewai)\n",
            "  Downloading marshmallow-3.21.2-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain<0.2.0,>=0.1.10->crewai)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.6->opentelemetry-api<2.0.0,>=1.22.0->crewai) (1.14.1)\n",
            "Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.98->crewai) (2.11.1)\n",
            "Requirement already satisfied: google-auth<3.0.0dev,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.98->crewai) (2.27.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.98->crewai) (1.23.0)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.98->crewai) (24.0)\n",
            "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.98->crewai) (2.8.0)\n",
            "Requirement already satisfied: google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.98->crewai) (3.21.0)\n",
            "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.98->crewai) (1.12.3)\n",
            "Requirement already satisfied: shapely<3.0.0dev in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.98->crewai) (2.0.4)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (from gptcache<0.2.0,>=0.1.43->embedchain<0.2.0,>=0.1.98->crewai) (5.3.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.13.3->crewai) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.13.3->crewai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.13.3->crewai) (0.14.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<=7.0,>=6.0->opentelemetry-api<2.0.0,>=1.22.0->crewai) (3.18.2)\n",
            "Collecting cohere<6.0,>=5.5 (from langchain-cohere<0.2.0,>=0.1.4->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading cohere-5.5.3-py3-none-any.whl (166 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.6/166.6 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jsonpatch<2.0,>=1.33 (from langchain-core<0.2.0,>=0.1.52->langchain<0.2.0,>=0.1.10->crewai)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting packaging>=14.3 (from google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog<4.0.0,>=3.0.2->embedchain<0.2.0,>=0.1.98->crewai) (1.16.0)\n",
            "Collecting monotonic>=1.5 (from posthog<4.0.0,>=3.0.2->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog<4.0.0,>=3.0.2->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: python-dateutil>2.1 in /usr/local/lib/python3.10/dist-packages (from posthog<4.0.0,>=3.0.2->embedchain<0.2.0,>=0.1.98->crewai) (2.8.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain<0.2.0,>=0.1.10->crewai) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain<0.2.0,>=0.1.10->crewai) (2.0.7)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=13.7.0->embedchain<0.2.0,>=0.1.98->crewai) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=13.7.0->embedchain<0.2.0,>=0.1.98->crewai) (2.16.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy<3.0.0,>=2.0.27->embedchain<0.2.0,>=0.1.98->crewai) (3.0.3)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb<0.6.0,>=0.5.0->embedchain<0.2.0,>=0.1.98->crewai) (1.1.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb<0.6.0,>=0.5.0->embedchain<0.2.0,>=0.1.98->crewai) (2.0.1)\n",
            "Collecting boto3<2.0.0,>=1.34.0 (from cohere<6.0,>=5.5->langchain-cohere<0.2.0,>=0.1.4->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading boto3-1.34.113-py3-none-any.whl (139 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastavro<2.0.0,>=1.9.4 (from cohere<6.0,>=5.5->langchain-cohere<0.2.0,>=0.1.4->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading fastavro-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx-sse<0.5.0,>=0.4.0 (from cohere<6.0,>=5.5->langchain-cohere<0.2.0,>=0.1.4->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Collecting types-requests<3.0.0,>=2.0.0 (from cohere<6.0,>=5.5->langchain-cohere<0.2.0,>=0.1.4->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading types_requests-2.32.0.20240523-py3-none-any.whl (15 kB)\n",
            "Collecting starlette<0.38.0,>=0.37.2 (from fastapi>=0.95.2->chromadb<0.6.0,>=0.5.0->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading starlette-0.37.2-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi-cli>=0.0.2 (from fastapi>=0.95.2->chromadb<0.6.0,>=0.5.0->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading fastapi_cli-0.0.4-py3-none-any.whl (9.5 kB)\n",
            "Requirement already satisfied: jinja2>=2.11.2 in /usr/local/lib/python3.10/dist-packages (from fastapi>=0.95.2->chromadb<0.6.0,>=0.5.0->embedchain<0.2.0,>=0.1.98->crewai) (3.1.4)\n",
            "Collecting python-multipart>=0.0.7 (from fastapi>=0.95.2->chromadb<0.6.0,>=0.5.0->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
            "Collecting ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 (from fastapi>=0.95.2->chromadb<0.6.0,>=0.5.0->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading ujson-5.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting email_validator>=2.0.0 (from fastapi>=0.95.2->chromadb<0.6.0,>=0.5.0->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading email_validator-2.1.1-py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.98->crewai) (1.48.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.98->crewai) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.98->crewai) (4.9)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.98->crewai) (2.3.3)\n",
            "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.98->crewai) (2.7.0)\n",
            "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /usr/local/lib/python3.10/dist-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.98->crewai) (0.13.0)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.52->langchain<0.2.0,>=0.1.10->crewai)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb<0.6.0,>=0.5.0->embedchain<0.2.0,>=0.1.98->crewai) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb<0.6.0,>=0.5.0->embedchain<0.2.0,>=0.1.98->crewai) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb<0.6.0,>=0.5.0->embedchain<0.2.0,>=0.1.98->crewai) (3.2.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.7.0->embedchain<0.2.0,>=0.1.98->crewai) (0.1.2)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb<0.6.0,>=0.5.0->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb<0.6.0,>=0.5.0->embedchain<0.2.0,>=0.1.98->crewai) (24.3.25)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb<0.6.0,>=0.5.0->embedchain<0.2.0,>=0.1.98->crewai) (1.12)\n",
            "Collecting opentelemetry-instrumentation-asgi==0.45b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.6.0,>=0.5.0->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading opentelemetry_instrumentation_asgi-0.45b0-py3-none-any.whl (14 kB)\n",
            "Collecting opentelemetry-instrumentation==0.45b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.6.0,>=0.5.0->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading opentelemetry_instrumentation-0.45b0-py3-none-any.whl (28 kB)\n",
            "Collecting opentelemetry-util-http==0.45b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.6.0,>=0.5.0->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading opentelemetry_util_http-0.45b0-py3-none-any.whl (6.9 kB)\n",
            "Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.45b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.6.0,>=0.5.0->embedchain<0.2.0,>=0.1.98->crewai) (67.7.2)\n",
            "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.45b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.6.0,>=0.5.0->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.2->chromadb<0.6.0,>=0.5.0->embedchain<0.2.0,>=0.1.98->crewai) (0.23.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain<0.2.0,>=0.1.10->crewai)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Collecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb<0.6.0,>=0.5.0->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb<0.6.0,>=0.5.0->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb<0.6.0,>=0.5.0->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading watchfiles-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb<0.6.0,>=0.5.0->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic<2.0.0,>=1.13.1->embedchain<0.2.0,>=0.1.98->crewai) (2.1.5)\n",
            "Collecting botocore<1.35.0,>=1.34.113 (from boto3<2.0.0,>=1.34.0->cohere<6.0,>=5.5->langchain-cohere<0.2.0,>=0.1.4->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading botocore-1.34.113-py3-none-any.whl (12.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3<2.0.0,>=1.34.0->cohere<6.0,>=5.5->langchain-cohere<0.2.0,>=0.1.4->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3<2.0.0,>=1.34.0->cohere<6.0,>=5.5->langchain-cohere<0.2.0,>=0.1.4->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading s3transfer-0.10.1-py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.2/82.2 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dnspython>=2.0.0 (from email_validator>=2.0.0->fastapi>=0.95.2->chromadb<0.6.0,>=0.5.0->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading dnspython-2.6.1-py3-none-any.whl (307 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: pip is looking at multiple versions of fastapi-cli to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting fastapi-cli>=0.0.2 (from fastapi>=0.95.2->chromadb<0.6.0,>=0.5.0->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading fastapi_cli-0.0.3-py3-none-any.whl (9.2 kB)\n",
            "  Downloading fastapi_cli-0.0.2-py3-none-any.whl (9.1 kB)\n",
            "Collecting fastapi>=0.95.2 (from chromadb<0.6.0,>=0.5.0->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading fastapi-0.110.3-py3-none-any.whl (91 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.8/91.8 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.98->crewai) (1.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb<0.6.0,>=0.5.0->embedchain<0.2.0,>=0.1.98->crewai) (3.14.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb<0.6.0,>=0.5.0->embedchain<0.2.0,>=0.1.98->crewai) (2023.6.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.98->crewai) (0.6.0)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb<0.6.0,>=0.5.0->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb<0.6.0,>=0.5.0->embedchain<0.2.0,>=0.1.98->crewai) (1.3.0)\n",
            "Building wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53724 sha256=862dff99be3568e538c0673ff638e836c15ec28c23e207b303ee57d2227def32\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n",
            "Successfully built pypika\n",
            "Installing collected packages: schema, pypika, monotonic, mmh3, appdirs, websockets, uvloop, uvicorn, types-requests, python-dotenv, pysbd, pypdf, packaging, overrides, orjson, opentelemetry-util-http, opentelemetry-semantic-conventions, opentelemetry-proto, mypy-extensions, Mako, jsonpointer, jmespath, importlib-metadata, humanfriendly, httpx-sse, httptools, fastavro, docstring-parser, deprecated, chroma-hnswlib, bcrypt, backoff, asgiref, watchfiles, typing-inspect, tiktoken, starlette, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, marshmallow, jsonpatch, gptcache, coloredlogs, botocore, alembic, s3transfer, opentelemetry-sdk, opentelemetry-instrumentation, onnxruntime, langsmith, kubernetes, fastapi, dataclasses-json, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-http, opentelemetry-exporter-otlp-proto-grpc, langchain-core, instructor, boto3, opentelemetry-instrumentation-fastapi, langchain-text-splitters, langchain-openai, langchain-community, cohere, langchain-cohere, langchain, chromadb, embedchain, crewai\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.0\n",
            "    Uninstalling packaging-24.0:\n",
            "      Successfully uninstalled packaging-24.0\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib_metadata 7.1.0\n",
            "    Uninstalling importlib_metadata-7.1.0:\n",
            "      Successfully uninstalled importlib_metadata-7.1.0\n",
            "  Attempting uninstall: docstring-parser\n",
            "    Found existing installation: docstring_parser 0.16\n",
            "    Uninstalling docstring_parser-0.16:\n",
            "      Successfully uninstalled docstring_parser-0.16\n",
            "Successfully installed Mako-1.3.5 alembic-1.13.1 appdirs-1.4.4 asgiref-3.8.1 backoff-2.2.1 bcrypt-4.1.3 boto3-1.34.113 botocore-1.34.113 chroma-hnswlib-0.7.3 chromadb-0.5.0 cohere-5.5.3 coloredlogs-15.0.1 crewai-0.30.11 dataclasses-json-0.6.6 deprecated-1.2.14 docstring-parser-0.15 embedchain-0.1.104 fastapi-0.110.3 fastavro-1.9.4 gptcache-0.1.43 httptools-0.6.1 httpx-sse-0.4.0 humanfriendly-10.0 importlib-metadata-7.0.0 instructor-0.5.2 jmespath-1.0.1 jsonpatch-1.33 jsonpointer-2.4 kubernetes-29.0.0 langchain-0.1.20 langchain-cohere-0.1.5 langchain-community-0.0.38 langchain-core-0.1.52 langchain-openai-0.1.7 langchain-text-splitters-0.0.2 langsmith-0.1.63 marshmallow-3.21.2 mmh3-4.1.0 monotonic-1.6 mypy-extensions-1.0.0 onnxruntime-1.18.0 opentelemetry-api-1.24.0 opentelemetry-exporter-otlp-proto-common-1.24.0 opentelemetry-exporter-otlp-proto-grpc-1.24.0 opentelemetry-exporter-otlp-proto-http-1.24.0 opentelemetry-instrumentation-0.45b0 opentelemetry-instrumentation-asgi-0.45b0 opentelemetry-instrumentation-fastapi-0.45b0 opentelemetry-proto-1.24.0 opentelemetry-sdk-1.24.0 opentelemetry-semantic-conventions-0.45b0 opentelemetry-util-http-0.45b0 orjson-3.10.3 overrides-7.7.0 packaging-23.2 posthog-3.5.0 pypdf-4.2.0 pypika-0.48.9 pysbd-0.3.4 python-dotenv-1.0.1 s3transfer-0.10.1 schema-0.7.7 starlette-0.37.2 tiktoken-0.7.0 types-requests-2.32.0.20240523 typing-inspect-0.9.0 uvicorn-0.29.0 uvloop-0.19.0 watchfiles-0.21.0 websockets-12.0\n"
          ]
        }
      ],
      "source": [
        "!pip install crewai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_m7_gzAqnAk7",
        "outputId": "f24c19f3-58ec-4f8b-9740-4e9f9252f130"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting unstructured\n",
            "  Downloading unstructured-0.14.2-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from unstructured) (5.2.0)\n",
            "Collecting filetype (from unstructured)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Collecting python-magic (from unstructured)\n",
            "  Downloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.9.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from unstructured) (3.8.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from unstructured) (0.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from unstructured) (2.31.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.12.3)\n",
            "Collecting emoji (from unstructured)\n",
            "  Downloading emoji-2.12.1-py3-none-any.whl (431 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.4/431.4 kB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from unstructured) (0.6.6)\n",
            "Collecting python-iso639 (from unstructured)\n",
            "  Downloading python_iso639-2024.4.27-py3-none-any.whl (274 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.7/274.7 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langdetect (from unstructured)\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from unstructured) (1.25.2)\n",
            "Collecting rapidfuzz (from unstructured)\n",
            "  Downloading rapidfuzz-3.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: backoff in /usr/local/lib/python3.10/dist-packages (from unstructured) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.11.0)\n",
            "Collecting unstructured-client (from unstructured)\n",
            "  Downloading unstructured_client-0.22.0-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from unstructured) (1.14.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->unstructured) (2.5)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->unstructured) (3.21.2)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->unstructured) (0.9.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect->unstructured) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (4.66.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured) (2024.2.2)\n",
            "Collecting deepdiff>=6.0 (from unstructured-client->unstructured)\n",
            "  Downloading deepdiff-7.0.1-py3-none-any.whl (80 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.8/80.8 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jsonpath-python>=1.0.6 (from unstructured-client->unstructured)\n",
            "  Downloading jsonpath_python-1.0.6-py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: mypy-extensions>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (1.0.0)\n",
            "Requirement already satisfied: packaging>=23.1 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (23.2)\n",
            "Requirement already satisfied: pypdf>=4.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (4.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (2.8.2)\n",
            "Collecting ordered-set<4.2.0,>=4.1.0 (from deepdiff>=6.0->unstructured-client->unstructured)\n",
            "  Downloading ordered_set-4.1.0-py3-none-any.whl (7.6 kB)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993227 sha256=0d674c3c2f3183e931f406caf45b414372015bae0f1fca88e391f2f6f370d3e9\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "Successfully built langdetect\n",
            "Installing collected packages: filetype, rapidfuzz, python-magic, python-iso639, ordered-set, langdetect, jsonpath-python, emoji, deepdiff, unstructured-client, unstructured\n",
            "Successfully installed deepdiff-7.0.1 emoji-2.12.1 filetype-1.2.0 jsonpath-python-1.0.6 langdetect-1.0.9 ordered-set-4.1.0 python-iso639-2024.4.27 python-magic-0.4.27 rapidfuzz-3.9.1 unstructured-0.14.2 unstructured-client-0.22.0\n"
          ]
        }
      ],
      "source": [
        "!pip install unstructured"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_bBjlk-nErZ",
        "outputId": "45237238-d28c-4eb9-ceb9-60caa7e2592f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langgraph\n",
            "  Downloading langgraph-0.0.55-py3-none-any.whl (84 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/84.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m81.9/84.1 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.1.20)\n",
            "Requirement already satisfied: langchain_openai in /usr/local/lib/python3.10/dist-packages (0.1.7)\n",
            "Collecting langchainhub\n",
            "  Downloading langchainhub-0.1.16-py3-none-any.whl (4.8 kB)\n",
            "Requirement already satisfied: langchain-core in /usr/local/lib/python3.10/dist-packages (0.1.52)\n",
            "Collecting langchain_experimental\n",
            "  Downloading langchain_experimental-0.0.59-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.5/199.5 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core\n",
            "  Downloading langchain_core-0.2.1-py3-none-any.whl (308 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m308.5/308.5 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uuid6<2025.0.0,>=2024.1.12 (from langgraph)\n",
            "  Downloading uuid6-2024.1.12-py3-none-any.whl (6.4 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.30)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.6.6)\n",
            "Requirement already satisfied: langchain-community<0.1,>=0.0.38 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.38)\n",
            "INFO: pip is looking at multiple versions of langchain to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.2.1-py3-none-any.whl (973 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m973.5/973.5 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.2.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.63)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.7.1)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.3.0)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.24.0 in /usr/local/lib/python3.10/dist-packages (from langchain_openai) (1.30.3)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.10/dist-packages (from langchain_openai) (0.7.0)\n",
            "Requirement already satisfied: types-requests<3.0.0.0,>=2.31.0.2 in /usr/local/lib/python3.10/dist-packages (from langchainhub) (2.32.0.20240523)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core) (1.33)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core) (23.2)\n",
            "Collecting langchain-community<0.3,>=0.2 (from langchain_experimental)\n",
            "  Downloading langchain_community-0.2.1-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core) (2.4)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.3)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.24.0->langchain_openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.24.0->langchain_openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.24.0->langchain_openai) (0.27.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.24.0->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.24.0->langchain_openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.24.0->langchain_openai) (4.11.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.18.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2023.12.25)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.24.0->langchain_openai) (1.2.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.21.2)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.24.0->langchain_openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.24.0->langchain_openai) (0.14.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
            "Installing collected packages: uuid6, langchainhub, langchain-core, langgraph, langchain-text-splitters, langchain, langchain-community, langchain_experimental\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.1.52\n",
            "    Uninstalling langchain-core-0.1.52:\n",
            "      Successfully uninstalled langchain-core-0.1.52\n",
            "  Attempting uninstall: langchain-text-splitters\n",
            "    Found existing installation: langchain-text-splitters 0.0.2\n",
            "    Uninstalling langchain-text-splitters-0.0.2:\n",
            "      Successfully uninstalled langchain-text-splitters-0.0.2\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.1.20\n",
            "    Uninstalling langchain-0.1.20:\n",
            "      Successfully uninstalled langchain-0.1.20\n",
            "  Attempting uninstall: langchain-community\n",
            "    Found existing installation: langchain-community 0.0.38\n",
            "    Uninstalling langchain-community-0.0.38:\n",
            "      Successfully uninstalled langchain-community-0.0.38\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "crewai 0.30.11 requires langchain<0.2.0,>=0.1.10, but you have langchain 0.2.1 which is incompatible.\n",
            "embedchain 0.1.104 requires langchain<0.2.0,>=0.1.4, but you have langchain 0.2.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed langchain-0.2.1 langchain-community-0.2.1 langchain-core-0.2.1 langchain-text-splitters-0.2.0 langchain_experimental-0.0.59 langchainhub-0.1.16 langgraph-0.0.55 uuid6-2024.1.12\n"
          ]
        }
      ],
      "source": [
        "!pip install langgraph langchain langchain_openai langchainhub langchain-core langchain_experimental"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "et1N1I-snLxA",
        "outputId": "956705fb-8d92-478a-8fad-c9ae3b594c88"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting sec_api\n",
            "  Downloading sec_api-1.0.18-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from sec_api) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->sec_api) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->sec_api) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->sec_api) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->sec_api) (2024.2.2)\n",
            "Installing collected packages: sec_api\n",
            "Successfully installed sec_api-1.0.18\n"
          ]
        }
      ],
      "source": [
        "!pip install sec_api"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1zEYTDlG2iP",
        "outputId": "4684dc57-8a34-40c3-d483-7ea8437f1141"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain_groq\n",
            "  Downloading langchain_groq-0.1.4-py3-none-any.whl (11 kB)\n",
            "Collecting groq<1,>=0.4.1 (from langchain_groq)\n",
            "  Downloading groq-0.8.0-py3-none-any.whl (105 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: langchain-core<0.3,>=0.1.45 in /usr/local/lib/python3.10/dist-packages (from langchain_groq) (0.2.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain_groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from groq<1,>=0.4.1->langchain_groq) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain_groq) (0.27.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain_groq) (2.7.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain_groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain_groq) (4.11.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.1.45->langchain_groq) (6.0.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.1.45->langchain_groq) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.1.45->langchain_groq) (0.1.63)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.1.45->langchain_groq) (23.2)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.1.45->langchain_groq) (8.3.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain_groq) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain_groq) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.1.45->langchain_groq) (2.4)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.3,>=0.1.45->langchain_groq) (3.10.3)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.3,>=0.1.45->langchain_groq) (2.31.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain_groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain_groq) (2.18.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.0->langchain-core<0.3,>=0.1.45->langchain_groq) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.0->langchain-core<0.3,>=0.1.45->langchain_groq) (2.0.7)\n",
            "Installing collected packages: groq, langchain_groq\n",
            "Successfully installed groq-0.8.0 langchain_groq-0.1.4\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain_groq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBv2qwKEGp18",
        "outputId": "32381004-cac3-4287-db60-b948dd1305af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: groq in /usr/local/lib/python3.10/dist-packages (0.8.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from groq) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from groq) (0.27.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from groq) (2.7.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from groq) (4.11.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq) (2.18.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install groq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Yp-I-elhFmoz",
        "outputId": "a8babb51-ae33-4db8-cba7-32a688154d41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/saeid976/researcher\n",
            "  Cloning https://github.com/saeid976/researcher to /tmp/pip-req-build-6mx1ov8k\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/saeid976/researcher /tmp/pip-req-build-6mx1ov8k\n",
            "  Resolved https://github.com/saeid976/researcher to commit 5757d943a71a6bfef9371fc0fddf7c3e712c3062\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting PyMuPDF>=1.23.6 (from gpt-researcher==0.0.5)\n",
            "  Downloading PyMuPDF-1.24.4-cp310-none-manylinux2014_x86_64.whl (3.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: SQLAlchemy>=2.0.28 in /usr/local/lib/python3.10/dist-packages (from gpt-researcher==0.0.5) (2.0.30)\n",
            "Collecting aiofiles>=23.2.1 (from gpt-researcher==0.0.5)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Collecting arxiv>=2.0.0 (from gpt-researcher==0.0.5)\n",
            "  Downloading arxiv-2.1.0-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: beautifulsoup4>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from gpt-researcher==0.0.5) (4.12.3)\n",
            "Collecting colorama>=0.4.6 (from gpt-researcher==0.0.5)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Collecting duckduckgo_search>=4.1.1 (from gpt-researcher==0.0.5)\n",
            "  Downloading duckduckgo_search-6.1.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: fastapi>=0.104.1 in /usr/local/lib/python3.10/dist-packages (from gpt-researcher==0.0.5) (0.110.3)\n",
            "Collecting htmldocx<0.0.7,>=0.0.6 (from gpt-researcher==0.0.5)\n",
            "  Downloading htmldocx-0.0.6-py3-none-any.whl (9.5 kB)\n",
            "Requirement already satisfied: jinja2>=3.1.2 in /usr/local/lib/python3.10/dist-packages (from gpt-researcher==0.0.5) (3.1.4)\n",
            "Requirement already satisfied: langchain>=0.0.350 in /usr/local/lib/python3.10/dist-packages (from gpt-researcher==0.0.5) (0.2.1)\n",
            "Collecting langchain-google-genai<0.0.12,>=0.0.11 (from gpt-researcher==0.0.5)\n",
            "  Downloading langchain_google_genai-0.0.11-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: langchain-openai<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from gpt-researcher==0.0.5) (0.1.7)\n",
            "Requirement already satisfied: langchain_community>=0.0.28 in /usr/local/lib/python3.10/dist-packages (from gpt-researcher==0.0.5) (0.2.1)\n",
            "Requirement already satisfied: langgraph>=0.0.29 in /usr/local/lib/python3.10/dist-packages (from gpt-researcher==0.0.5) (0.0.55)\n",
            "Requirement already satisfied: lxml[html-clean]>=4.9.2 in /usr/local/lib/python3.10/dist-packages (from gpt-researcher==0.0.5) (4.9.4)\n",
            "Requirement already satisfied: markdown>=3.5.1 in /usr/local/lib/python3.10/dist-packages (from gpt-researcher==0.0.5) (3.6)\n",
            "Collecting md2pdf>=1.0.1 (from gpt-researcher==0.0.5)\n",
            "  Downloading md2pdf-1.0.1.tar.gz (6.4 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting mistune<4.0.0,>=3.0.2 (from gpt-researcher==0.0.5)\n",
            "  Downloading mistune-3.0.2-py3-none-any.whl (47 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.0/48.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting newspaper3k>=0.2.8 (from gpt-researcher==0.0.5)\n",
            "  Downloading newspaper3k-0.2.8-py3-none-any.whl (211 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.1/211.1 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: openai>=1.3.3 in /usr/local/lib/python3.10/dist-packages (from gpt-researcher==0.0.5) (1.30.3)\n",
            "Collecting permchain>=0.0.6 (from gpt-researcher==0.0.5)\n",
            "  Downloading permchain-0.0.8-py3-none-any.whl (21 kB)\n",
            "Collecting playwright>=1.39.0 (from gpt-researcher==0.0.5)\n",
            "  Downloading playwright-1.44.0-py3-none-manylinux1_x86_64.whl (37.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.8/37.8 MB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic>=2.5.1 in /usr/local/lib/python3.10/dist-packages (from gpt-researcher==0.0.5) (2.7.1)\n",
            "Collecting python-docx<2.0.0,>=1.1.0 (from gpt-researcher==0.0.5)\n",
            "  Downloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dotenv>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from gpt-researcher==0.0.5) (1.0.1)\n",
            "Collecting python-multipart>=0.0.6 (from gpt-researcher==0.0.5)\n",
            "  Using cached python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: pyyaml>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from gpt-researcher==0.0.5) (6.0.1)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from gpt-researcher==0.0.5) (2.31.0)\n",
            "Collecting tavily-python>=0.2.8 (from gpt-researcher==0.0.5)\n",
            "  Downloading tavily_python-0.3.3-py3-none-any.whl (5.4 kB)\n",
            "Requirement already satisfied: uvicorn>=0.24.0.post1 in /usr/local/lib/python3.10/dist-packages (from gpt-researcher==0.0.5) (0.29.0)\n",
            "Collecting feedparser==6.0.10 (from arxiv>=2.0.0->gpt-researcher==0.0.5)\n",
            "  Downloading feedparser-6.0.10-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.1/81.1 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->gpt-researcher==0.0.5) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->gpt-researcher==0.0.5) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->gpt-researcher==0.0.5) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->gpt-researcher==0.0.5) (2024.2.2)\n",
            "Collecting sgmllib3k (from feedparser==6.0.10->arxiv>=2.0.0->gpt-researcher==0.0.5)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.12.2->gpt-researcher==0.0.5) (2.5)\n",
            "Requirement already satisfied: click>=8.1.7 in /usr/local/lib/python3.10/dist-packages (from duckduckgo_search>=4.1.1->gpt-researcher==0.0.5) (8.1.7)\n",
            "Collecting pyreqwest-impersonate>=0.4.5 (from duckduckgo_search>=4.1.1->gpt-researcher==0.0.5)\n",
            "  Downloading pyreqwest_impersonate-0.4.5-cp38-abi3-manylinux_2_28_x86_64.whl (2.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m71.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: orjson>=3.10.3 in /usr/local/lib/python3.10/dist-packages (from duckduckgo_search>=4.1.1->gpt-researcher==0.0.5) (3.10.3)\n",
            "Requirement already satisfied: starlette<0.38.0,>=0.37.2 in /usr/local/lib/python3.10/dist-packages (from fastapi>=0.104.1->gpt-researcher==0.0.5) (0.37.2)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from fastapi>=0.104.1->gpt-researcher==0.0.5) (4.11.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=3.1.2->gpt-researcher==0.0.5) (2.1.5)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.350->gpt-researcher==0.0.5) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.350->gpt-researcher==0.0.5) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.350->gpt-researcher==0.0.5) (0.2.1)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.350->gpt-researcher==0.0.5) (0.2.0)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.350->gpt-researcher==0.0.5) (0.1.63)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.350->gpt-researcher==0.0.5) (1.25.2)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.350->gpt-researcher==0.0.5) (8.3.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain_community>=0.0.28->gpt-researcher==0.0.5) (0.6.6)\n",
            "Collecting google-generativeai<0.5.0,>=0.4.1 (from langchain-google-genai<0.0.12,>=0.0.11->gpt-researcher==0.0.5)\n",
            "  Downloading google_generativeai-0.4.1-py3-none-any.whl (137 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.4/137.4 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: pip is looking at multiple versions of langchain-google-genai to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting langchain_community>=0.0.28 (from gpt-researcher==0.0.5)\n",
            "  Using cached langchain_community-0.2.1-py3-none-any.whl (2.1 MB)\n",
            "  Downloading langchain_community-0.2.0-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m45.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Using cached langchain_community-0.0.38-py3-none-any.whl (2.0 MB)\n",
            "INFO: pip is looking at multiple versions of langchain-community to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading langchain_community-0.0.37-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m57.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading langchain_community-0.0.36-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m59.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading langchain_community-0.0.35-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m53.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading langchain_community-0.0.34-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading langchain_community-0.0.33-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m56.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading langchain_community-0.0.32-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m56.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading langchain_community-0.0.31-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: pip is looking at multiple versions of langchain-community to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading langchain_community-0.0.30-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading langchain_community-0.0.29-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m72.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading langchain_community-0.0.28-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain>=0.0.350 (from gpt-researcher==0.0.5)\n",
            "  Using cached langchain-0.2.1-py3-none-any.whl (973 kB)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.0->langchain>=0.0.350->gpt-researcher==0.0.5) (1.33)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.0->langchain>=0.0.350->gpt-researcher==0.0.5) (23.2)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading langchain-0.2.0-py3-none-any.whl (973 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m973.7/973.7 kB\u001b[0m \u001b[31m53.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: pip is looking at multiple versions of langchain-google-genai to determine which version is compatible with other requirements. This could take a while.\n",
            "  Using cached langchain-0.1.20-py3-none-any.whl (1.0 MB)\n",
            "Collecting langchain-core<0.2.0,>=0.1.52 (from langchain>=0.0.350->gpt-researcher==0.0.5)\n",
            "  Using cached langchain_core-0.1.52-py3-none-any.whl (302 kB)\n",
            "Collecting langchain-text-splitters<0.1,>=0.0.1 (from langchain>=0.0.350->gpt-researcher==0.0.5)\n",
            "  Using cached langchain_text_splitters-0.0.2-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.10/dist-packages (from langchain-openai<0.2.0,>=0.1.0->gpt-researcher==0.0.5) (0.7.0)\n",
            "INFO: pip is looking at multiple versions of langgraph to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting langgraph>=0.0.29 (from gpt-researcher==0.0.5)\n",
            "  Downloading langgraph-0.0.54-py3-none-any.whl (83 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.9/83.9 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading langgraph-0.0.53-py3-none-any.whl (83 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.8/83.8 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading langgraph-0.0.52-py3-none-any.whl (83 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.8/83.8 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading langgraph-0.0.51-py3-none-any.whl (83 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.5/83.5 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: uuid6<2025.0.0,>=2024.1.12 in /usr/local/lib/python3.10/dist-packages (from langgraph>=0.0.29->gpt-researcher==0.0.5) (2024.1.12)\n",
            "\u001b[33mWARNING: lxml 4.9.4 does not provide the extra 'html-clean'\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting markdown2 (from md2pdf>=1.0.1->gpt-researcher==0.0.5)\n",
            "  Downloading markdown2-2.4.13-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docopt (from md2pdf>=1.0.1->gpt-researcher==0.0.5)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting WeasyPrint (from md2pdf>=1.0.1->gpt-researcher==0.0.5)\n",
            "  Downloading weasyprint-62.1-py3-none-any.whl (289 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.1/289.1 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from newspaper3k>=0.2.8->gpt-researcher==0.0.5) (9.4.0)\n",
            "Collecting cssselect>=0.9.2 (from newspaper3k>=0.2.8->gpt-researcher==0.0.5)\n",
            "  Downloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.10/dist-packages (from newspaper3k>=0.2.8->gpt-researcher==0.0.5) (3.8.1)\n",
            "Collecting tldextract>=2.0.1 (from newspaper3k>=0.2.8->gpt-researcher==0.0.5)\n",
            "  Downloading tldextract-5.1.2-py3-none-any.whl (97 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.6/97.6 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting feedfinder2>=0.0.4 (from newspaper3k>=0.2.8->gpt-researcher==0.0.5)\n",
            "  Downloading feedfinder2-0.0.4.tar.gz (3.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jieba3k>=0.35.1 (from newspaper3k>=0.2.8->gpt-researcher==0.0.5)\n",
            "  Downloading jieba3k-0.35.1.zip (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m67.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from newspaper3k>=0.2.8->gpt-researcher==0.0.5) (2.8.2)\n",
            "Collecting tinysegmenter==0.3 (from newspaper3k>=0.2.8->gpt-researcher==0.0.5)\n",
            "  Downloading tinysegmenter-0.3.tar.gz (16 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.3.3->gpt-researcher==0.0.5) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.3.3->gpt-researcher==0.0.5) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.3.3->gpt-researcher==0.0.5) (0.27.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>=1.3.3->gpt-researcher==0.0.5) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai>=1.3.3->gpt-researcher==0.0.5) (4.66.4)\n",
            "Requirement already satisfied: greenlet==3.0.3 in /usr/local/lib/python3.10/dist-packages (from playwright>=1.39.0->gpt-researcher==0.0.5) (3.0.3)\n",
            "Collecting pyee==11.1.0 (from playwright>=1.39.0->gpt-researcher==0.0.5)\n",
            "  Downloading pyee-11.1.0-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.5.1->gpt-researcher==0.0.5) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.5.1->gpt-researcher==0.0.5) (2.18.2)\n",
            "Collecting PyMuPDFb==1.24.3 (from PyMuPDF>=1.23.6->gpt-researcher==0.0.5)\n",
            "  Downloading PyMuPDFb-1.24.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (15.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.8/15.8 MB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.24.0.post1->gpt-researcher==0.0.5) (0.14.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.350->gpt-researcher==0.0.5) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.350->gpt-researcher==0.0.5) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.350->gpt-researcher==0.0.5) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.350->gpt-researcher==0.0.5) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.350->gpt-researcher==0.0.5) (1.9.4)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=1.3.3->gpt-researcher==0.0.5) (1.2.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community>=0.0.28->gpt-researcher==0.0.5) (3.21.2)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community>=0.0.28->gpt-researcher==0.0.5) (0.9.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from feedfinder2>=0.0.4->newspaper3k>=0.2.8->gpt-researcher==0.0.5) (1.16.0)\n",
            "Collecting google-ai-generativelanguage==0.4.0 (from google-generativeai<0.5.0,>=0.4.1->langchain-google-genai<0.0.12,>=0.0.11->gpt-researcher==0.0.5)\n",
            "  Downloading google_ai_generativelanguage-0.4.0-py3-none-any.whl (598 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m598.7/598.7 kB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.5.0,>=0.4.1->langchain-google-genai<0.0.12,>=0.0.11->gpt-researcher==0.0.5) (2.27.0)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.5.0,>=0.4.1->langchain-google-genai<0.0.12,>=0.0.11->gpt-researcher==0.0.5) (2.11.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.5.0,>=0.4.1->langchain-google-genai<0.0.12,>=0.0.11->gpt-researcher==0.0.5) (3.20.3)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-ai-generativelanguage==0.4.0->google-generativeai<0.5.0,>=0.4.1->langchain-google-genai<0.0.12,>=0.0.11->gpt-researcher==0.0.5) (1.23.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai>=1.3.3->gpt-researcher==0.0.5) (1.0.5)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3k>=0.2.8->gpt-researcher==0.0.5) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3k>=0.2.8->gpt-researcher==0.0.5) (2023.12.25)\n",
            "Collecting requests-file>=1.4 (from tldextract>=2.0.1->newspaper3k>=0.2.8->gpt-researcher==0.0.5)\n",
            "  Downloading requests_file-2.1.0-py2.py3-none-any.whl (4.2 kB)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.10/dist-packages (from tldextract>=2.0.1->newspaper3k>=0.2.8->gpt-researcher==0.0.5) (3.14.0)\n",
            "Collecting pydyf>=0.10.0 (from WeasyPrint->md2pdf>=1.0.1->gpt-researcher==0.0.5)\n",
            "  Downloading pydyf-0.10.0-py3-none-any.whl (8.1 kB)\n",
            "Requirement already satisfied: cffi>=0.6 in /usr/local/lib/python3.10/dist-packages (from WeasyPrint->md2pdf>=1.0.1->gpt-researcher==0.0.5) (1.16.0)\n",
            "Requirement already satisfied: html5lib>=1.1 in /usr/local/lib/python3.10/dist-packages (from WeasyPrint->md2pdf>=1.0.1->gpt-researcher==0.0.5) (1.1)\n",
            "Requirement already satisfied: tinycss2>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from WeasyPrint->md2pdf>=1.0.1->gpt-researcher==0.0.5) (1.3.0)\n",
            "Collecting cssselect2>=0.1 (from WeasyPrint->md2pdf>=1.0.1->gpt-researcher==0.0.5)\n",
            "  Downloading cssselect2-0.7.0-py3-none-any.whl (15 kB)\n",
            "Collecting Pyphen>=0.9.1 (from WeasyPrint->md2pdf>=1.0.1->gpt-researcher==0.0.5)\n",
            "  Downloading pyphen-0.15.0-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m71.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fonttools[woff]>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from WeasyPrint->md2pdf>=1.0.1->gpt-researcher==0.0.5) (4.51.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=0.6->WeasyPrint->md2pdf>=1.0.1->gpt-researcher==0.0.5) (2.22)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from cssselect2>=0.1->WeasyPrint->md2pdf>=1.0.1->gpt-researcher==0.0.5) (0.5.1)\n",
            "Collecting zopfli>=0.1.4 (from fonttools[woff]>=4.0.0->WeasyPrint->md2pdf>=1.0.1->gpt-researcher==0.0.5)\n",
            "  Downloading zopfli-0.2.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (848 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m848.9/848.9 kB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting brotli>=1.0.1 (from fonttools[woff]>=4.0.0->WeasyPrint->md2pdf>=1.0.1->gpt-researcher==0.0.5)\n",
            "  Downloading Brotli-1.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m65.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai<0.5.0,>=0.4.1->langchain-google-genai<0.0.12,>=0.0.11->gpt-researcher==0.0.5) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai<0.5.0,>=0.4.1->langchain-google-genai<0.0.12,>=0.0.11->gpt-researcher==0.0.5) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai<0.5.0,>=0.4.1->langchain-google-genai<0.0.12,>=0.0.11->gpt-researcher==0.0.5) (4.9)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain>=0.0.350->gpt-researcher==0.0.5) (2.4)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community>=0.0.28->gpt-researcher==0.0.5) (1.0.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai<0.5.0,>=0.4.1->langchain-google-genai<0.0.12,>=0.0.11->gpt-researcher==0.0.5) (1.63.0)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai<0.5.0,>=0.4.1->langchain-google-genai<0.0.12,>=0.0.11->gpt-researcher==0.0.5) (1.64.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai<0.5.0,>=0.4.1->langchain-google-genai<0.0.12,>=0.0.11->gpt-researcher==0.0.5) (1.48.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai<0.5.0,>=0.4.1->langchain-google-genai<0.0.12,>=0.0.11->gpt-researcher==0.0.5) (0.6.0)\n",
            "Building wheels for collected packages: gpt-researcher, md2pdf, tinysegmenter, feedfinder2, jieba3k, docopt, sgmllib3k\n",
            "  Building wheel for gpt-researcher (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gpt-researcher: filename=gpt_researcher-0.0.5-py3-none-any.whl size=46155 sha256=29c792c121908dd7f1b29bf58a49220798f05187999cc95329625e5aa4c213e0\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-7qbzd7d9/wheels/a2/3e/bc/afd6c8b7970c68d8c678b09835ef459545c6904c242ccc2b96\n",
            "  Building wheel for md2pdf (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for md2pdf: filename=md2pdf-1.0.1-py2.py3-none-any.whl size=5998 sha256=32dbec3efa04cc570e28f5f4d6dc298f10e4089aec0d74d253fd0c742997f768\n",
            "  Stored in directory: /root/.cache/pip/wheels/c9/81/b8/90aedc679b3c06471f744e360636b8d08bc104d38c11fdfbca\n",
            "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-py3-none-any.whl size=13538 sha256=e05f0584b4a88d4dc5d81f6a2e534b367d4cfb1a09e81370f43eb671fed674a7\n",
            "  Stored in directory: /root/.cache/pip/wheels/c8/d6/6c/384f58df48c00b9a31d638005143b5b3ac62c3d25fb1447f23\n",
            "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-py3-none-any.whl size=3340 sha256=f50ce0186fec6974ee7f822ff48ad13edb2acf3b10f2b27dd7d017901e597e11\n",
            "  Stored in directory: /root/.cache/pip/wheels/97/02/e7/a1ff1760e12bdbaab0ac824fae5c1bc933e41c4ccd6a8f8edb\n",
            "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jieba3k: filename=jieba3k-0.35.1-py3-none-any.whl size=7398382 sha256=e68cdc57c403f3472b9d2a8a9102b9b5867f7948cec2aaf7161f3035657c509b\n",
            "  Stored in directory: /root/.cache/pip/wheels/7a/c4/0c/12a9a314ecac499456c4c3b2fcc2f635a3b45a39dfbd240299\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=266caf61ab600a3a5e3a3cd23845e5d3384840d0dce2c211f5ce2267b9a7b30e\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6049 sha256=2ffa70adeb373b07a94e8f672b01163fb4738c66dbff1d804598b0d17d8f3ef5\n",
            "  Stored in directory: /root/.cache/pip/wheels/f0/69/93/a47e9d621be168e9e33c7ce60524393c0b92ae83cf6c6e89c5\n",
            "Successfully built gpt-researcher md2pdf tinysegmenter feedfinder2 jieba3k docopt sgmllib3k\n",
            "Installing collected packages: tinysegmenter, sgmllib3k, jieba3k, docopt, brotli, zopfli, python-multipart, python-docx, pyreqwest-impersonate, Pyphen, PyMuPDFb, pyee, pydyf, mistune, markdown2, feedparser, cssselect, colorama, aiofiles, requests-file, PyMuPDF, playwright, htmldocx, feedfinder2, duckduckgo_search, cssselect2, arxiv, WeasyPrint, tldextract, tavily-python, newspaper3k, md2pdf, langchain-core, langgraph, langchain-text-splitters, langchain_community, google-ai-generativelanguage, langchain, google-generativeai, permchain, langchain-google-genai, gpt-researcher\n",
            "  Attempting uninstall: mistune\n",
            "    Found existing installation: mistune 0.8.4\n",
            "    Uninstalling mistune-0.8.4:\n",
            "      Successfully uninstalled mistune-0.8.4\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.2.1\n",
            "    Uninstalling langchain-core-0.2.1:\n",
            "      Successfully uninstalled langchain-core-0.2.1\n",
            "  Attempting uninstall: langgraph\n",
            "    Found existing installation: langgraph 0.0.55\n",
            "    Uninstalling langgraph-0.0.55:\n",
            "      Successfully uninstalled langgraph-0.0.55\n",
            "  Attempting uninstall: langchain-text-splitters\n",
            "    Found existing installation: langchain-text-splitters 0.2.0\n",
            "    Uninstalling langchain-text-splitters-0.2.0:\n",
            "      Successfully uninstalled langchain-text-splitters-0.2.0\n",
            "  Attempting uninstall: langchain_community\n",
            "    Found existing installation: langchain-community 0.2.1\n",
            "    Uninstalling langchain-community-0.2.1:\n",
            "      Successfully uninstalled langchain-community-0.2.1\n",
            "  Attempting uninstall: google-ai-generativelanguage\n",
            "    Found existing installation: google-ai-generativelanguage 0.6.4\n",
            "    Uninstalling google-ai-generativelanguage-0.6.4:\n",
            "      Successfully uninstalled google-ai-generativelanguage-0.6.4\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.2.1\n",
            "    Uninstalling langchain-0.2.1:\n",
            "      Successfully uninstalled langchain-0.2.1\n",
            "  Attempting uninstall: google-generativeai\n",
            "    Found existing installation: google-generativeai 0.5.4\n",
            "    Uninstalling google-generativeai-0.5.4:\n",
            "      Successfully uninstalled google-generativeai-0.5.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain-experimental 0.0.59 requires langchain-community<0.3,>=0.2, but you have langchain-community 0.0.38 which is incompatible.\n",
            "langchain-experimental 0.0.59 requires langchain-core<0.3,>=0.2, but you have langchain-core 0.1.52 which is incompatible.\n",
            "nbconvert 6.5.4 requires mistune<2,>=0.8.1, but you have mistune 3.0.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed PyMuPDF-1.24.4 PyMuPDFb-1.24.3 Pyphen-0.15.0 WeasyPrint-62.1 aiofiles-23.2.1 arxiv-2.1.0 brotli-1.1.0 colorama-0.4.6 cssselect-1.2.0 cssselect2-0.7.0 docopt-0.6.2 duckduckgo_search-6.1.0 feedfinder2-0.0.4 feedparser-6.0.10 google-ai-generativelanguage-0.4.0 google-generativeai-0.4.1 gpt-researcher-0.0.5 htmldocx-0.0.6 jieba3k-0.35.1 langchain-0.1.20 langchain-core-0.1.52 langchain-google-genai-0.0.11 langchain-text-splitters-0.0.2 langchain_community-0.0.38 langgraph-0.0.51 markdown2-2.4.13 md2pdf-1.0.1 mistune-3.0.2 newspaper3k-0.2.8 permchain-0.0.8 playwright-1.44.0 pydyf-0.10.0 pyee-11.1.0 pyreqwest-impersonate-0.4.5 python-docx-1.1.2 python-multipart-0.0.9 requests-file-2.1.0 sgmllib3k-1.0.0 tavily-python-0.3.3 tinysegmenter-0.3 tldextract-5.1.2 zopfli-0.2.3\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "9f913a497c4041a986a4a78fb497733e",
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install git+https://github.com/saeid976/researcher"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVaURl9SEzMm"
      },
      "source": [
        "# Function_call notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "Q40lkyLGnjWy"
      },
      "outputs": [],
      "source": [
        "# %run functions_python.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "QVnAbvAnZIi-"
      },
      "outputs": [],
      "source": [
        "# fc = FunctionCalls()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "chkHPIgMZATp"
      },
      "outputs": [],
      "source": [
        "import functions_python\n",
        "import input_filter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "sqCrGtqPn8zd"
      },
      "outputs": [],
      "source": [
        "# fc = functions_python.FunctionCalls()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScRdMcIrHYZX"
      },
      "source": [
        "# Langgraph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3e30lMEBHFcy"
      },
      "outputs": [],
      "source": [
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from langchain.tools import BaseTool\n",
        "from langgraph.prebuilt import ToolExecutor\n",
        "from typing import Optional, Type"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlPLaNe4GjgH"
      },
      "source": [
        "## On boarding:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "446r9He4Go5K"
      },
      "outputs": [],
      "source": [
        "# !pip install psycopg2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gCKEDbC-ogr2"
      },
      "outputs": [],
      "source": [
        "# import psycopg2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gPAJPkmMGsbX"
      },
      "outputs": [],
      "source": [
        "# class SaveTradingStyle(BaseModel):\n",
        "#     \"\"\"save the style of trading.\"\"\"\n",
        "\n",
        "#     userId: float = Field(..., description=\"user id\")\n",
        "\n",
        "\n",
        "# def trading_style_saver(userId: float) -> str:\n",
        "#     try:\n",
        "#         # Connect to the PostgreSQL server\n",
        "#         conn = psycopg2.connect(\n",
        "#             dbname=\"tensurf\",\n",
        "#             user=\"tensurf\",\n",
        "#             password=\"tensurf\",\n",
        "#             host=\"135.181.158.245\",\n",
        "#             port=\"5432\"\n",
        "#         )\n",
        "\n",
        "#         # Create a cursor object\n",
        "#         cur = conn.cursor()\n",
        "\n",
        "#         trading_style = input(\"What is your trading style (day trader, swing trader, or both)? \")\n",
        "#         cur.execute(\"SELECT COUNT(*) FROM onboarding_user WHERE id = %s\",(userId,))\n",
        "#         count = cur.fetchone()[0]\n",
        "#         if count > 0:\n",
        "#             cur.execute(\"UPDATE onboarding_user SET trading_style=%s WHERE id=%s\",(trading_style, userId))\n",
        "#         else:\n",
        "#             cur.execute(\"INSERT INTO onboarding_user (id,trading_style) values (%s,%s)\", (userId, trading_style))\n",
        "\n",
        "#         conn.commit()\n",
        "#         cur.close()\n",
        "#         conn.close()\n",
        "\n",
        "#         return \"Trading style is saved successfully.\"\n",
        "\n",
        "#     except psycopg2.Error as e:\n",
        "#         return f\"Error in trading_style_saver: {e}\"\n",
        "\n",
        "\n",
        "# databaseStyle = StructuredTool.from_function(\n",
        "#     func=trading_style_saver,\n",
        "#     name=\"SaveTradingStyle\",\n",
        "#     description=\"save the style of trading\",\n",
        "#     args_schema=SaveTradingStyle,\n",
        "#     return_direct=False,\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pMvPszfGG0VH"
      },
      "outputs": [],
      "source": [
        "# class SaveRiskPercentage(BaseModel):\n",
        "#     \"\"\"Get the info of the trader about how much risk they take for trading.\"\"\"\n",
        "\n",
        "#     userId: float = Field(..., description=\"user id\")\n",
        "\n",
        "# def risk_percentage_saver(userId) -> str:\n",
        "#     # Save risk percentage to the database\n",
        "#     try:\n",
        "#         # Connect to the PostgreSQL server\n",
        "#         conn = psycopg2.connect(\n",
        "#             dbname=\"tensurf\",\n",
        "#             user=\"tensurf\",\n",
        "#             password=\"tensurf\",\n",
        "#             host=\"135.181.158.245\",\n",
        "#             port=\"5432\"\n",
        "#         )\n",
        "\n",
        "#         # Create a cursor object\n",
        "#         cur = conn.cursor()\n",
        "\n",
        "#         cur.execute(\"SELECT COUNT(*) FROM onboarding_user WHERE id = %s\",(userId,))\n",
        "#         while True:\n",
        "#             percentage = float(input(\"What is the maximum percentage of your capital that you want to risk in trade (0-100)? \"))\n",
        "#             if 0 <= percentage <= 100:\n",
        "#                 break\n",
        "#             else:\n",
        "#                 print(\"Please enter a positive float number between 0 and 100.\")\n",
        "#         count = cur.fetchone()[0]\n",
        "#         if count > 0:\n",
        "#             cur.execute(\"UPDATE onboarding_user SET risk_parameter=%s WHERE id=%s\",(percentage, userId))\n",
        "#         else:\n",
        "#             cur.execute(\"INSERT INTO onboarding_user (id,risk_parameter) values (%s,%s)\", (userId, percentage))\n",
        "\n",
        "#         # Commit the transaction\n",
        "#         conn.commit()\n",
        "#         cur.close()\n",
        "#         conn.close()\n",
        "\n",
        "#         return \"percentage of risk is saved successfully.\"\n",
        "\n",
        "#     except psycopg2.Error as e:\n",
        "#         return f\"Error in risk_percentage_saver: {e}\"\n",
        "\n",
        "\n",
        "# databaseRisk = StructuredTool.from_function(\n",
        "#     func=risk_percentage_saver,\n",
        "#     name=\"SaveRiskPercentage\",\n",
        "#     description=\"Get the info of the trader about how much risk they take for trading.\",\n",
        "#     args_schema=SaveRiskPercentage,\n",
        "#     return_direct=False,\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5mB3KvXDG06h"
      },
      "outputs": [],
      "source": [
        "# def check_user_id():\n",
        "#     try:\n",
        "#         # Connect to the PostgreSQL server\n",
        "#         conn = psycopg2.connect(\n",
        "#             dbname=\"tensurf\",\n",
        "#             user=\"tensurf\",\n",
        "#             password=\"tensurf\",\n",
        "#             host=\"135.181.158.245\",\n",
        "#             port=\"5432\"\n",
        "#         )\n",
        "\n",
        "#         # Create a cursor object\n",
        "#         cur = conn.cursor()\n",
        "#         user_id = input(\"What is your id?\")\n",
        "#         # Query to check if the user_id exists in the table\n",
        "#         cur.execute(\"SELECT COUNT(*) FROM onboarding_user WHERE id = %s\",(user_id,))\n",
        "#         count = cur.fetchone()[0]\n",
        "\n",
        "#         if count > 0:\n",
        "#           return user_id\n",
        "\n",
        "#         else:\n",
        "#           return f\"User with user_id= {user_id} does not exist in the table.\"\n",
        "\n",
        "#     except psycopg2.Error as e:\n",
        "#       return f\"Error in check_user_id: {e}\"\n",
        "\n",
        "# databaseId = StructuredTool.from_function(\n",
        "#     func=check_user_id,\n",
        "#     name=\"CheckUserId\",\n",
        "#     description=\"Checking the id of the user\",\n",
        "#     return_direct=False,\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "behgQf7V1wNk"
      },
      "source": [
        "## Calculate Stop Loss:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "enrMEMS7HaTe"
      },
      "outputs": [],
      "source": [
        "class PropertiesCalculateSl(BaseModel):\n",
        "\tsymbol: Optional[str] = Field(None, description=\"The ticker symbol of the financial instrument to be analyzed.\")\n",
        "\n",
        "\tmethod: Optional[str] = Field(None, description=\"shows the method of SL calculation.\")\n",
        "\n",
        "\tdirection: Optional[int] = Field(None, description=\"-1: means the user want to calculate stoploss for a short position. 1: means the user want to calculate stoploss for a long position\")\n",
        "\n",
        "\tlookback: Optional[int] = Field(None, description=\"it is used when the method is set to 'minmax' and shows the number of candles that the SL is calculated based on them.\")\n",
        "\n",
        "\tneighborhood: Optional[int] = Field(None, description=\"A parameter that is used in the swing method to define the range or window within which swings are detected. example: \\\n",
        "If the 'neighborhood' parameter is set to 3, it means that the swing detection is based on considering 3 candles to the \\\n",
        "left and 3 candles to the right of the swing point.\")\n",
        "\n",
        "\tatr_coef: Optional[int] = Field(None, description=\"it is used if the method is 'atr' and shows the coefficient of atr\")\n",
        "\n",
        "\n",
        "class CalculateSL(BaseTool):\n",
        "\tname = \"calculate_sl\"\n",
        "\tdescription = \"\"\"Stoploss (SL) is a limitation for potential losses in a position. It's below the current price for long position and above it for short position. \\\n",
        "Distance between the SL and current price is named risk value. This function calculates the SL based o some different methods. \\\n",
        "Returns A dictionary same as this: \\\n",
        "{'sl': [17542.5], 'risk': [268.5], 'info': ['calculated based on maximum high price of previous 100 candles']} \\\n",
        "which includes sl value, risk on the trade and an information. \\\n",
        "If user don't select any method for sl calculation or select \"level\" method, or zigzag method the otput can include \\\n",
        "more than one stoploss and the values type in the output can be a list such as this \\\n",
        "{'sl': [17542.5, 17818.25, 17858.5, 17882.5, 18518.75], 'risk': [268.5, 7.25, 47.5, 71.5, 707.75], 'info': ['minmax', 'swing', 'atr', '5min_SR', 'daily_SR']} \\\n",
        "It includes a list of stoplosses and the risk on them and finally the level or method name of stoploss.\"\"\"\n",
        "\n",
        "\targs_schema: Type[BaseModel] = PropertiesCalculateSl\n",
        "\n",
        "\tdef _run(\n",
        "        self, symbol: str = None, method: str = None, direction: int = None,\n",
        "\t\tlookback: int = None, neighborhood: int = None, atr_coef: int = None\n",
        "    ) -> int:\n",
        "\t\tparameters = {\n",
        "                      \"symbol\": symbol,\n",
        "                      \"method\": method,\n",
        "                      \"direction\": direction,\n",
        "                      \"lookback\": lookback,\n",
        "                      \"neighborhood\": neighborhood,\n",
        "                      \"atr_coef\": atr_coef\n",
        "                      }\n",
        "\n",
        "\t\tfc = functions_python.FunctionCalls()\n",
        "\t\t\n",
        "\t\treturn fc.calculate_sl(parameters)\n",
        "\n",
        "SL = CalculateSL()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EciRRD4YELCo"
      },
      "source": [
        "## Calculate Take-Profit:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "JianU1wXGZL3"
      },
      "outputs": [],
      "source": [
        "class PropertiesCalculateTp(BaseModel):\n",
        "\tsymbol: Optional[str] = Field(None, description=\"The ticker symbol of the financial instrument to be analyzed.\")\n",
        "\n",
        "\tdirection: Optional[int] = Field(None, description=\"-1: means the user want to calculate stoploss for a short position. 1: means the user want to calculate stoploss for a long position\")\n",
        "\n",
        "\tstoploss: Optional[int] = Field(None, description=\"the value for stoploss\")\n",
        "\n",
        "\n",
        "class CalculateTp(BaseTool):\n",
        "\tname = \"calculate_tp\"\n",
        "\tdescription = \"\"\"Take profit (TP) is opposite of the stop-loss (SL) and is based on maximum reward that we intend to achieve from a trade. \\\n",
        "It represents the price level at which a trader aims to close a position to secure profits before the market reverses. \\\n",
        "Returns list of price for take-profit and information for each price For exampe: \\\n",
        "{'tp': [5139.25, 5140.25, 5144.0], 'info': ['calculated based on the level VWAP_Top_Band_2', 'calculated based on the level Overnight_high', 'calculated based on the level VWAP_Top_Band_3']}\"\"\"\n",
        "\n",
        "\targs_schema: Type[BaseModel] = PropertiesCalculateTp\n",
        "\n",
        "\tdef _run(\n",
        "        self, symbol: str = None, direction: int = None, stoploss: int = None\n",
        "    ) -> int:\n",
        "\t\tparameters = {\n",
        "\t\t\t\t\t\t\"symbol\": symbol,\n",
        "\t\t\t\t\t\t\"direction\": direction,\n",
        "\t\t\t\t\t\t\"stoploss\": stoploss\n",
        "\t\t\t\t\t\t}\n",
        "\n",
        "\t\tfc = functions_python.FunctionCalls()\n",
        "\t\t\n",
        "\t\treturn fc.calculate_tp(parameters)\n",
        "\n",
        "TP = CalculateTp()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lt4sSmAQH1Ph"
      },
      "source": [
        "## Trend Detection:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ZM38gBSZEjBk"
      },
      "outputs": [],
      "source": [
        "class PropertiesCalculateTrend(BaseModel):\n",
        "\tsymbol: Optional[str] = Field(None, description=\"The ticker symbol of the financial instrument to be analyzed.\")\n",
        "\n",
        "\tstart_datetime: Optional[str] = Field(None, description=\"The start timestamp of period over which the analysis is done. \\\n",
        "\tThe format of the date should be in the following format %b-%d-%y %H:%M:%S like this example: May-1-2024 13:27:49\")\n",
        "\tend_datetime: Optional[str] = Field(None, description=\"The end timestamp of period over which the analysis is done. \\\n",
        "\tThe format of the date should be in the following format %b-%d-%y %H:%M:%S like this example: May-1-2024 13:27:49. \\\n",
        "\tThe user can set this parameter to now. In this situation this parameter's value is the current date time.\")\n",
        "\n",
        "\tlookback: Optional[str] = Field(None, description=\"The number of seconds, minutes, hours, days, weeks, months or years to look back for calculating the trend of the given symbol. \\\n",
        "This parameter determines the depth of historical data to be considered in the analysis. The format of this value must obey one of the following examples: 30 seconds, 10 minutes, 2 hours, 5 days, 3 weeks, 2 months and 3 years. \\\n",
        "Either start_datetime along with end_datetime should be specified or lookback should be specified but both cases should not happen simultaneously.\")\n",
        "\n",
        "class CalculateTrend(BaseTool):\n",
        "\tname = \"detect_trend\"\n",
        "\tdescription = \"\"\"Analyzes the trend of a specified financial instrument over a given time range. \\\n",
        "It is designed primarily for financial data analysis, enabling users to gauge the general direction of a security or market index. \\\n",
        "Whether start_datetime with end_datetime, end_datetime with lookback or lookback parameters could be valued for determining the period over which's trend wants to be detected. \\\n",
        "The function returns a numerical value that indicates the trend intensity and direction within the specified parameters. \\\n",
        "Returns a number between -3 and 3 that represents the trend’s intensity and direction. The value is interpreted as follows: \\\n",
        "\\n -3: strong bearish (downward) trend \\\n",
        "\\n -2: moderate bearish (downward) trend \\\n",
        "\\n -1: mild bearish (downward) trend \\\n",
        "\\n 0: without significant trend \\\n",
        "\\n 3: strong bullish (upward) trend \\\n",
        "\\n 2: moderate bullish (upward) trend \\\n",
        "\\n 1: mild bullish (upward) trend\"\"\"\n",
        "\n",
        "\targs_schema: Type[BaseModel] = PropertiesCalculateTrend\n",
        "\n",
        "\tdef _run(\n",
        "\t\t\tself, symbol: str = None, start_datetime: str = None, end_datetime: str = None, lookback: str = None\n",
        "    ) -> dict:\n",
        "\n",
        "\t\tfunction_arguments = {\n",
        "\t\t\t\t\t\t\"symbol\": symbol,\n",
        "\t\t\t\t\t\t\"start_datetime\": start_datetime,\n",
        "\t\t\t\t\t\t\"end_datetime\": end_datetime,\n",
        "\t\t\t\t\t\t\"lookback\": lookback\n",
        "\t\t\t\t\t\t}\n",
        "\n",
        "\t\tfc = functions_python.FunctionCalls()\n",
        "\t\t\n",
        "\t\treturn fc.detect_trend(function_arguments)\n",
        "\n",
        "\n",
        "Trend = CalculateTrend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AahZLwrkH-G-"
      },
      "source": [
        "## Calculate Support and Resistance Levels:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Gv2RBUw3H6s_"
      },
      "outputs": [],
      "source": [
        "class PropertiesCalculateSR(BaseModel):\n",
        "\tsymbol: Optional[str] = Field(None, description=\"The ticker symbol of the financial instrument to be analyzed.\")\n",
        "\ttimeframe: Optional[str] = Field(None, description=\"Specifies the timeframe of the candlestick chart to be analyzed. \\\n",
        "\tThis parameter defines the granularity of the data used for calculating the levels. The only allowed formats would like 3h, 20min, 1d.\")\n",
        "\tlookback_days: Optional[str] = Field(None, description=\"The number of days to look back for calculating the support and resistance levels. \\\n",
        "This parameter determines the depth of historical data to be considered in the analysis. (e.g. 10 days)\")\n",
        "\n",
        "class CalculateSR(BaseTool):\n",
        "\tname = \"calculate_sr\"\n",
        "\tdescription = \"\"\"Support and resistance levels represent price points on a chart where the odds favor a pause or reversal of a prevailing trend. \\\n",
        "This function analyzes candlestick charts over a specified timeframe and lookback period to calculate these levels and their respective strengths. \\\n",
        "Returns a dictionary containing five lists, each corresponding to a specific aspect of the calculated support and resistance levels: \\\n",
        "1. levels_prices (list of floats): The prices at which support and resistance levels have been identified. \\\n",
        "2. levels_start_timestamps (list of timestamps) \\\n",
        "3. levels_detect_timestamps (list of timestamps) \\\n",
        "4. levels_end_timestamps (list of timestamps) \\\n",
        "5. levels_scores (list of floats): Scores associated with each level, indicating the strength or significance of the level. Higher scores typically imply stronger levels.\"\"\"\n",
        "\n",
        "\targs_schema: Type[BaseModel] = PropertiesCalculateSR\n",
        "\n",
        "\tdef _run(\n",
        "\t\t\tself, symbol: str = None, timeframe: str = None, lookback_days: str = None\n",
        "    ) -> dict:\n",
        "\t\tparameters = {\n",
        "\t\t\t\t\t\t\"symbol\": symbol,\n",
        "\t\t\t\t\t\t\"timeframe\": timeframe,\n",
        "\t\t\t\t\t\t\"lookback_days\": lookback_days\n",
        "\t\t\t\t\t\t}\n",
        "\n",
        "\t\tfc = functions_python.FunctionCalls()\n",
        "\t\t\n",
        "\t\treturn fc.calculate_sr(parameters)\n",
        "\n",
        "SR = CalculateSR()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALEtGCoWoI_2"
      },
      "source": [
        "## Bias detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Xj0NnY7OoRtF"
      },
      "outputs": [],
      "source": [
        "class PropertiesBiasDetection(BaseModel):\n",
        "\tsymbol: Optional[str] = Field(None, description=\"The ticker symbol of the financial instrument to be analyzed.\")\n",
        "\n",
        "\tmethod: Optional[str] = Field(None, description=\"The user can choose from different methods including MC, Zigzag trend, \\\n",
        "Trend detection, weekly wvap, candle stick pattern, cross ma, vp detection ,power & counter ratio.\")\n",
        "\n",
        "class BiasDetection(BaseTool):\n",
        "\tname = \"bias_detection\"\n",
        "\tdescription = \"\"\"Detecting trading bias through different methods or Detecting the appropriate entry point for a long or short trade.\n",
        "Returns a number between -3 and 3 that represents the trend’s intensity and direction. The value is interpreted as follows:\n",
        "-3: Strong downward , -2: downward -1: Weak downward, 0: No significant trend / Neutral, 1: Weak upward, 2.upward, 3: Strong upward\"\"\"\n",
        "\n",
        "\targs_schema: Type[BaseModel] = PropertiesBiasDetection\n",
        "\n",
        "\tdef _run(\n",
        "        self, symbol: str = None, method: str = None\n",
        "    ) -> dict:\n",
        "\n",
        "\t\tparameters = {\n",
        "\t\t\t\t\t\t\"symbol\": symbol,\n",
        "\t\t\t\t\t\t\"method\": method\n",
        "\t\t\t\t\t\t}\n",
        "\n",
        "\t\tfc = functions_python.FunctionCalls()\n",
        "\t\t\n",
        "\t\treturn fc.bias_detection(parameters)\n",
        "\n",
        "Bias = BiasDetection()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwjkTlbXlsgC"
      },
      "source": [
        "## Financial tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "id": "tPZezZDc4HaZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['AZURE_EMBEDDING_MODEL'] ='embedding-ada-002'\n",
        "os.environ['LLM_PROVIDER'] = 'groq'\n",
        "os.environ[\"AZURE_OPENAI_API_KEY\"] =  \"80ddd1ad72504f2fa226755d49491a61\"\n",
        "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://tensurfbrain1.openai.azure.com/\"\n",
        "os.environ[\"OPENAI_API_VERSION\"] = \"2023-10-01-preview\"\n",
        "os.environ['EMBEDDING_PROVIDER']='azureopenai'\n",
        "os.environ['SMART_LLM_MODEL']='llama3-70b-8192'\n",
        "os.environ['TAVILY_API_KEY']='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'\n",
        "os.environ['GROQ_API_KEY'] = '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'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "id": "6BLvYUSu9cJs",
        "outputId": "f897ed80-3169-4aa4-db66-d31d6baaead4"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'gpt_researcher'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[125], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgpt_researcher\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GPTResearcher\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01masyncio\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdotenv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gpt_researcher'"
          ]
        }
      ],
      "source": [
        "from gpt_researcher import GPTResearcher\n",
        "import asyncio\n",
        "from dotenv import load_dotenv\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "2pgbq2vu9c2W",
        "outputId": "d4ac3024-7897-441b-8ff1-88a7886dc60e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔎 Starting the research task for 'what day is today?'...\n",
            "Default Agent\n"
          ]
        },
        {
          "ename": "JSONDecodeError",
          "evalue": "Extra data: line 3 column 1 (char 92)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-612ebf24bbf9>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mresearcher\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGPTResearcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mreport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mresearcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconduct_research\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mreport\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gpt_researcher/master/agent.py\u001b[0m in \u001b[0;36mconduct_research\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_context_by_urls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msource_urls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_context_by_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gpt_researcher/master/agent.py\u001b[0m in \u001b[0;36mget_context_by_search\u001b[0;34m(self, query)\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;31m# Generate Sub-Queries including original query\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0msub_queries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mget_sub_queries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrole\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent_query\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreport_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;31m# If this is not part of a sub researcher, add original query to research for better results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gpt_researcher/master/functions.py\u001b[0m in \u001b[0;36mget_sub_queries\u001b[0;34m(query, agent_role_prompt, cfg, parent_query, report_type)\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0mllm_provider\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm_provider\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     )\n\u001b[0;32m--> 104\u001b[0;31m     \u001b[0msub_queries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msub_queries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Extra data\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mJSONDecodeError\u001b[0m: Extra data: line 3 column 1 (char 92)"
          ]
        }
      ],
      "source": [
        "query = \"what day is today?\"\n",
        "report_type = \"research_report\"\n",
        "\n",
        "researcher = GPTResearcher(query, report_type)\n",
        "report = await researcher.conduct_research()\n",
        "report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "7_Y5zF-Ol0oi",
        "outputId": "b2582d80-7491-4691-bfb0-4385732f30f8"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-d632b85a3e32>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m               \u001b[0;32mimport\u001b[0m \u001b[0mAzureOpenAIEmbeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_community\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorstores\u001b[0m   \u001b[0;32mimport\u001b[0m \u001b[0mFAISS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0munstructured\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhtml\u001b[0m        \u001b[0;32mimport\u001b[0m \u001b[0mpartition_html\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msec_api\u001b[0m                            \u001b[0;32mimport\u001b[0m \u001b[0mQueryApi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/unstructured/partition/html.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0munstructured\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunking\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0madd_chunking_strategy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0munstructured\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melements\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mElement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocess_metadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0munstructured\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhtml\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHTMLDocument\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0munstructured\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxml\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVALID_PARSERS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0munstructured\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mread_txt_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/unstructured/documents/html.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlxml\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0metree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m from unstructured.cleaners.core import (\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mclean_bullets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mreplace_unicode_quotes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/unstructured/cleaners/core.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m from unstructured.file_utils.encoding import (\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mformat_encoding_str\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/unstructured/file_utils/encoding.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mchardet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0munstructured\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconvert_to_bytes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mENCODE_REC_THRESHOLD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/unstructured/partition/common.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mTitle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m )\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0munstructured\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0munstructured\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatterns\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mENUMERATED_BULLETS_RE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUNICODE_BULLETS_RE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0munstructured\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstants\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSORT_MODE_DONT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSORT_MODE_XY_CUT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/unstructured/logger.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Note(Trevor,Crag): to opt out of scarf analytics, set the environment variable:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# SCARF_NO_ANALYTICS=true. See the README for more info.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mscarf_analytics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Add the custom log method to the logging.Logger class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/unstructured/utils.py\u001b[0m in \u001b[0;36mscarf_analytics\u001b[0;34m()\u001b[0m\n\u001b[1;32m    347\u001b[0m                 )\n\u001b[1;32m    348\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m                 requests.get(\n\u001b[0m\u001b[1;32m    350\u001b[0m                     \u001b[0;34m\"https://packages.unstructured.io/python-telemetry?version=\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m                     \u001b[0;34m+\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \"\"\"\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"get\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    587\u001b[0m         }\n\u001b[1;32m    588\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    723\u001b[0m             \u001b[0;31m# Redirect resolving generator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m             \u001b[0mgen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolve_redirects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 725\u001b[0;31m             \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mresp\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mresp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m             \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    723\u001b[0m             \u001b[0;31m# Redirect resolving generator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m             \u001b[0mgen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolve_redirects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 725\u001b[0;31m             \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mresp\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mresp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m             \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mresolve_redirects\u001b[0;34m(self, resp, req, stream, timeout, verify, cert, proxies, yield_requests, **adapter_kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m                 resp = self.send(\n\u001b[0m\u001b[1;32m    267\u001b[0m                     \u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m                     \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m             resp = conn.urlopen(\n\u001b[0m\u001b[1;32m    487\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m                 \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m             \u001b[0;31m# Make the request on the HTTPConnection object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 791\u001b[0;31m             response = self._make_request(\n\u001b[0m\u001b[1;32m    792\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0;31m# Receive the response from the server\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 537\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    538\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m         \u001b[0;31m# Get the response from http.client.HTTPConnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 461\u001b[0;31m         \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1375\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1376\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1301\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1303\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1304\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1305\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1157\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1158\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1159\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1161\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from langchain.tools.yahoo_finance_news import YahooFinanceNewsTool\n",
        "from langchain.text_splitter            import CharacterTextSplitter\n",
        "from langchain.embeddings               import AzureOpenAIEmbeddings\n",
        "from langchain_community.vectorstores   import FAISS\n",
        "from unstructured.partition.html        import partition_html\n",
        "from sec_api                            import QueryApi\n",
        "import requests\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k1sZSRy4cGWU"
      },
      "outputs": [],
      "source": [
        "api_type = \"azure\"\n",
        "api_endpoint = 'https://tensurfbrain1.openai.azure.com/'\n",
        "api_version = '2023-10-01-preview'\n",
        "api_key = '80ddd1ad72504f2fa226755d49491a61'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uj86R_SODN7t"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from openai import AzureOpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IojgHYsIDP7j"
      },
      "outputs": [],
      "source": [
        "api_type = \"azure\"\n",
        "api_endpoint = 'https://tensurfbrain1.openai.azure.com/'\n",
        "api_version = '2023-10-01-preview'\n",
        "api_key = '80ddd1ad72504f2fa226755d49491a61'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z4wONgm_DpdO"
      },
      "outputs": [],
      "source": [
        "client = AzureOpenAI(\n",
        "    api_key=api_key,\n",
        "    api_version=api_version,\n",
        "    azure_endpoint=api_endpoint\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qNPoFrgXDVvZ"
      },
      "outputs": [],
      "source": [
        "def generate_embeddings(text, model='embedding-ada-002'):\n",
        "  return client.embeddings.create(input = [text], model=model).data[0].embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXLc0iHYZ48v"
      },
      "source": [
        "### Search tools:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "280xJBiDmFrH"
      },
      "outputs": [],
      "source": [
        "class PropertiesSearchTheInternet(BaseModel):\n",
        "    query: str = Field(..., description=\"A random topic that will be searched on the internet\")\n",
        "\n",
        "\n",
        "class SearchTheInternet(BaseTool):\n",
        "    name = \"SearchTheInternet\"\n",
        "    description = \"\"\"Useful to search the internet about a a given topic and return relevant results\"\"\"\n",
        "\n",
        "    args_schema: Type[BaseModel] = PropertiesSearchTheInternet\n",
        "\n",
        "    def _run(\n",
        "        self, query: str\n",
        "    ) -> dict:\n",
        "            top_result_to_return = 4\n",
        "            url = \"https://google.serper.dev/search\"\n",
        "            payload = json.dumps({\"q\": query})\n",
        "            headers = {\n",
        "                'X-API-KEY': '77976766163a61a4bae1ed8672ae79e916955783',\n",
        "                'content-type': 'application/json'\n",
        "            }\n",
        "            response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
        "            results = response.json()['organic']\n",
        "            string = []\n",
        "            for result in results[:top_result_to_return]:\n",
        "              try:\n",
        "                string.append('\\n'.join([\n",
        "                    f\"Title: {result['title']}\", f\"Link: {result['link']}\",\n",
        "                    f\"Snippet: {result['snippet']}\", \"\\n-----------------\"\n",
        "                ]))\n",
        "              except KeyError:\n",
        "                next\n",
        "\n",
        "            return '\\n'.join(string)\n",
        "\n",
        "SearchInternet = SearchTheInternet()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6Ou8CTPZIp9"
      },
      "outputs": [],
      "source": [
        "SearchInternet.run(\"Apple\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EawXSHEOXRFq"
      },
      "outputs": [],
      "source": [
        "class PropertiesSearchNews(BaseModel):\n",
        "    query: str = Field(..., description=\"A random topic that will be searched on the internet\")\n",
        "\n",
        "\n",
        "class SearchNews(BaseTool):\n",
        "    name = \"SearchNews\"\n",
        "    description = \"\"\"Useful to search news about a company, stock or any other topic and return relevant results\"\"\"\n",
        "\n",
        "    args_schema: Type[BaseModel] = PropertiesSearchNews\n",
        "\n",
        "    def _run(\n",
        "        self, query: str\n",
        "    ) -> dict:\n",
        "            top_result_to_return = 4\n",
        "            url = \"https://google.serper.dev/news\"\n",
        "            payload = json.dumps({\"q\": query})\n",
        "            headers = {\n",
        "                'X-API-KEY': '77976766163a61a4bae1ed8672ae79e916955783',\n",
        "                'content-type': 'application/json'\n",
        "            }\n",
        "            response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
        "            results = response.json()['news']\n",
        "            string = []\n",
        "            for result in results[:top_result_to_return]:\n",
        "              try:\n",
        "                string.append('\\n'.join([\n",
        "                    f\"Title: {result['title']}\", f\"Link: {result['link']}\",\n",
        "                    f\"Snippet: {result['snippet']}\", \"\\n-----------------\"\n",
        "                ]))\n",
        "              except KeyError:\n",
        "                next\n",
        "\n",
        "            return '\\n'.join(string)\n",
        "\n",
        "NewsSearch = SearchNews()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nNVEG3bmZzf0"
      },
      "outputs": [],
      "source": [
        "NewsSearch.run(\"NQ\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etCFxbh3Z8iv"
      },
      "source": [
        "### SECTools:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TvUHS5EHbzb2"
      },
      "outputs": [],
      "source": [
        "def download_form_html(url):\n",
        "  headers = {\n",
        "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
        "    'Accept-Encoding': 'gzip, deflate, br',\n",
        "    'Accept-Language': 'en-US,en;q=0.9,pt-BR;q=0.8,pt;q=0.7',\n",
        "    'Cache-Control': 'max-age=0',\n",
        "    'Dnt': '1',\n",
        "    'Sec-Ch-Ua': '\"Not_A Brand\";v=\"8\", \"Chromium\";v=\"120\"',\n",
        "    'Sec-Ch-Ua-Mobile': '?0',\n",
        "    'Sec-Ch-Ua-Platform': '\"macOS\"',\n",
        "    'Sec-Fetch-Dest': 'document',\n",
        "    'Sec-Fetch-Mode': 'navigate',\n",
        "    'Sec-Fetch-Site': 'none',\n",
        "    'Sec-Fetch-User': '?1',\n",
        "    'Upgrade-Insecure-Requests': '1',\n",
        "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\n",
        "  }\n",
        "\n",
        "  response = requests.get(url, headers=headers)\n",
        "\n",
        "  return response.text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pl1QuMSZb2sL"
      },
      "outputs": [],
      "source": [
        "def embedding_search(url, ask):\n",
        "  text = download_form_html(url)\n",
        "  elements = partition_html(text=text)\n",
        "  content = \"\\n\".join([str(el) for el in elements])\n",
        "  text_splitter = CharacterTextSplitter(\n",
        "      separator = \"\\n\",\n",
        "      chunk_size = 1000,\n",
        "      chunk_overlap  = 150,\n",
        "      length_function = len,\n",
        "      is_separator_regex = False,\n",
        "  )\n",
        "  docs = text_splitter.create_documents([content])\n",
        "  retriever = FAISS.from_documents(\n",
        "      docs, AzureOpenAIEmbeddings()\n",
        "    ).as_retriever()\n",
        "  retriever = FAISS.from_documents(\n",
        "    docs, client.embeddings\n",
        "  ).as_retriever()\n",
        "  answers = retriever.get_relevant_documents(ask, top_k=4)\n",
        "  answers = \"\\n\\n\".join([a.page_content for a in answers])\n",
        "\n",
        "  return answers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CbRpT87iaNZy"
      },
      "outputs": [],
      "source": [
        "class PropertiesSearch10q(BaseModel):\n",
        "    stock: str = Field(..., description=\"A random stock.\")\n",
        "    ask: str = Field(..., description=\"Question from the 10-Q of the stock\")\n",
        "\n",
        "\n",
        "class Search10q(BaseTool):\n",
        "    name = \"Search10q\"\n",
        "    description = \"\"\"Useful to search information from the latest 10-Q form for a given stock.\n",
        "                     The input to this tool should be a pipe (|) separated text of\n",
        "                     length two, representing the stock ticker you are interested and what\n",
        "                     question you have from it.\n",
        "                     For example, `AAPL|what was last quarter's revenue`.\"\"\"\n",
        "\n",
        "    args_schema: Type[BaseModel] = PropertiesSearch10q\n",
        "\n",
        "    def _run(\n",
        "        self, stock: str, ask: str\n",
        "    ) -> dict:\n",
        "            queryApi = QueryApi(api_key=\"befd094ddadf6a9e080bf194b6d9197e753ec1d4226fc8e595ed717b2f1bc3a5\")\n",
        "            query = {\n",
        "              \"query\": {\n",
        "                \"query_string\": {\n",
        "                  \"query\": f\"ticker:{stock} AND formType:\\\"10-Q\\\"\"\n",
        "                }\n",
        "              },\n",
        "              \"from\": \"0\",\n",
        "              \"size\": \"1\",\n",
        "              \"sort\": [{ \"filedAt\": { \"order\": \"desc\" }}]\n",
        "            }\n",
        "\n",
        "            fillings = queryApi.get_filings(query)['filings']\n",
        "            if len(fillings) == 0:\n",
        "              return \"Sorry, I couldn't find any filling for this stock, check if the ticker is correct.\"\n",
        "            link = fillings[0]['linkToFilingDetails']\n",
        "            answer = embedding_search(link, ask)\n",
        "            return answer\n",
        "\n",
        "TenQ = Search10q()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zpdtHTRAcJ72"
      },
      "outputs": [],
      "source": [
        "TenQ.run({\"stock\":\"AAPL\", \"ask\":\"what was last quarter's revenue.\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g4Qb73vldAkP"
      },
      "outputs": [],
      "source": [
        "class PropertiesSearch10k(BaseModel):\n",
        "    stock: str = Field(..., description=\"A random stock.\")\n",
        "    ask: str = Field(..., description=\"Question from the 10-k of the stock\")\n",
        "\n",
        "\n",
        "class Search10k(BaseTool):\n",
        "    name = \"Search10k\"\n",
        "    description = \"\"\"Useful to search information from the latest 10-K form for a given stock.\n",
        "                     The input to this tool should be a pipe (|) separated text of\n",
        "                     length two, representing the stock ticker you are interested, what\n",
        "                     question you have from it.\n",
        "                     For example, `AAPL|what was last year's revenue`.\"\"\"\n",
        "\n",
        "    args_schema: Type[BaseModel] = PropertiesSearch10k\n",
        "\n",
        "    def _run(\n",
        "        self, stock: str, ask: str\n",
        "    ) -> dict:\n",
        "            queryApi = QueryApi(api_key=\"befd094ddadf6a9e080bf194b6d9197e753ec1d4226fc8e595ed717b2f1bc3a5\")\n",
        "            query = {\n",
        "              \"query\": {\n",
        "                \"query_string\": {\n",
        "                  \"query\": f\"ticker:{stock} AND formType:\\\"10-K\\\"\"\n",
        "                }\n",
        "              },\n",
        "              \"from\": \"0\",\n",
        "              \"size\": \"1\",\n",
        "              \"sort\": [{ \"filedAt\": { \"order\": \"desc\" }}]\n",
        "            }\n",
        "\n",
        "            fillings = queryApi.get_filings(query)['filings']\n",
        "            if len(fillings) == 0:\n",
        "              return \"Sorry, I couldn't find any filling for this stock, check if the ticker is correct.\"\n",
        "            link = fillings[0]['linkToFilingDetails']\n",
        "            answer = embedding_search(link, ask)\n",
        "            return answer\n",
        "\n",
        "Tenk = Search10k()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhFk82hXFm2b"
      },
      "source": [
        "## Openai utilites"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "rivfZPicJUKb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from openai import AzureOpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "W16S_ST_JQA0"
      },
      "outputs": [],
      "source": [
        "api_type = \"azure\"\n",
        "api_endpoint = 'https://tensurfbrain1.openai.azure.com/'\n",
        "api_version = '2023-10-01-preview'\n",
        "api_key = '80ddd1ad72504f2fa226755d49491a61'\n",
        "client = AzureOpenAI(\n",
        "    api_key= api_key,\n",
        "    api_version= api_version,\n",
        "    azure_endpoint= api_endpoint\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "JHliSi6UsYzU"
      },
      "outputs": [],
      "source": [
        "class ChatWithOpenai:\n",
        "  def __init__(self, system_message, model, temperature, max_tokens, client, default_user_messages=None):\n",
        "    self.system_message = system_message\n",
        "    self.model = model\n",
        "    self.temperature = temperature\n",
        "    self.max_tokens = max_tokens\n",
        "    self.client = client\n",
        "    self.messages = [{\"role\": \"system\", \"content\": system_message}]\n",
        "    if default_user_messages:\n",
        "      for user_message in default_user_messages:\n",
        "        self.messages += [{\"role\": \"user\", \"content\": user_message}]\n",
        "\n",
        "  def chat(self, user_input):\n",
        "    # print(self.messages + user_input)\n",
        "    response = self.client.chat.completions.create(\n",
        "        model=self.model,\n",
        "        messages=self.messages + user_input,\n",
        "        temperature=self.temperature,\n",
        "        max_tokens=self.max_tokens\n",
        "    )\n",
        "    return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xr_s_UHkFGCT"
      },
      "source": [
        "## Irrelevant handler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "v5b3pyZJNYl3"
      },
      "outputs": [],
      "source": [
        "# message = \"'check the grammer for me?'\"\n",
        "# user_input = [{\"role\": \"user\", \"content\": message}]\n",
        "# handler_zero_openai.chat(user_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "T6nK6QWyOlx-"
      },
      "outputs": [],
      "source": [
        "# Handler.run(\"check the grammer for me?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "GvR7waNZrJSZ"
      },
      "outputs": [],
      "source": [
        "default_message = [\"Check the following text for greeting words and do as the system message said.\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ry7DgJSZQT0g"
      },
      "outputs": [],
      "source": [
        "handler_zero_openai = ChatWithOpenai(system_message=\"You are an assistant. Your job is to check the user input Not answer it.\\\n",
        "If the user input contains greeting words like ‘hello’, ‘hi’, and so on, \\\n",
        "then you should remove these words. \\\n",
        "Note that the final response is either the input with the greeting word removed, \\\n",
        "or ‘None’ if the input consists only of a greeting word. \\\n",
        "if there is no greeting word in the input, the response is the last user input itself \\\n",
        "without any changes. \\\n",
        "No other responses are possible. \\\n",
        "Here are some examples and the valid responses: \\\n",
        "Example 1 (when there is no greeting word): \\\n",
        "{ input: ‘What is the weather?’ response: ‘What is the weather?’ } \\\n",
        "Example 2 (when there is a greeting word): \\\n",
        "{ input: ‘Hi can you speak French?’ response: ‘Can you speak French?’ } \\\n",
        "Example 3 (when there is just one or more greeting words): \\\n",
        "{ input: ‘Hello.’ response: ‘None’ } \\\n",
        "Note that you are not permitted to answer user requests directly. \\\n",
        "Only perform the tasks instructed by system messages.\",\n",
        "\n",
        "                                      default_user_messages=default_message,\n",
        "                                      model=\"gpt_35_16k\",\n",
        "                                      temperature=0.2,\n",
        "                                      max_tokens=100,\n",
        "                                      client=client)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "dx8bLNJsCY0U"
      },
      "outputs": [],
      "source": [
        "handler_one_openai = ChatWithOpenai(system_message=\"You are an assistant. Your job is to check the user input, not answer it. \\\n",
        "We are a system with the name 'Tensurf Brain' or 'Tensurf'. \\\n",
        "If the user needs any information about us or needs a tutorial for \\\n",
        "how our system is working, your job is to detect these scenarios and respond with 'True'. \\\n",
        "If the user asks about you, the answer is also 'True'. \\\n",
        "Also, when the user is confused and doesn't know how to start or what to do, \\\n",
        "the answer is also 'True'. \\\n",
        "For any other input that doesn't classify in the tutorial or information \\\n",
        "about 'Tensurf Brain' or 'Tensurf' you should return false. \\\n",
        "Note that you are not permitted to answer user requests directly. \\\n",
        "Only perform the tasks instructed by system messages.\",\n",
        "                                    model=\"gpt_35_16k\",\n",
        "                                    temperature=0.2,\n",
        "                                    max_tokens=100,\n",
        "                                    client=client)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Deg_ndB8NhcO"
      },
      "outputs": [],
      "source": [
        "handler_two_openai = ChatWithOpenai(system_message=\"You are an assistant. Your job is to check the user input. \\\n",
        "If the user input does not contain any financial and \\\n",
        "trading topics or requests, then you should answer only\\\n",
        "with ‘True’. Otherwise, return ‘False’. \\\n",
        "Possible responses for you are 'True' or 'False'. \\\n",
        "For example, the correct response to the question \\\n",
        "'What is the trend?' is 'False'.\",\n",
        "                              model=\"gpt_35_16k\",\n",
        "                              temperature=0,\n",
        "                              max_tokens=100,\n",
        "                              client=client)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "J8UHKYBEEfH5"
      },
      "outputs": [],
      "source": [
        "class HandleIrrelevantSchema(BaseModel):\n",
        "    massage: str = Field(..., description=\"The humanmassage\")\n",
        "\n",
        "\n",
        "class HandleIrrelevant(BaseTool):\n",
        "    name = \"HandleIrrelevant\"\n",
        "    description = \"\"\"This function checks if the message contains financial or trading subjects or not. \\\n",
        "The output of this function is either True or False and the possible output of this function are 'True' or 'False'.: \\\n",
        "True: when the message contains financial or trading subjects. \\\n",
        "False: when the message request is not in these fields.\"\"\"\n",
        "\n",
        "    args_schema: Type[BaseModel] = HandleIrrelevantSchema\n",
        "\n",
        "    def _run(\n",
        "        self, massage: str\n",
        "    ) -> dict:\n",
        "\n",
        "        user_input = [{\"role\": \"user\", \"content\": \"'\" + massage + \"'\"}]\n",
        "\n",
        "        modified_input = handler_zero_openai.chat(user_input)\n",
        "        # print(f\"inside handler func: {modified_input}\")\n",
        "        if modified_input == 'None':\n",
        "          return 'Greeting'\n",
        "\n",
        "        modified_user_input = [{\"role\": \"user\", \"content\": modified_input}]\n",
        "\n",
        "        if handler_one_openai.chat(modified_user_input) == 'True':\n",
        "          return \"Tutorial\"\n",
        "        # print(f\"in the handler: {modified_user_input}\")\n",
        "        else:\n",
        "          return handler_two_openai.chat(modified_user_input)\n",
        "\n",
        "        # chat_with_openai(user_input)\n",
        "        # return chat_with_openai(user_input)\n",
        "\n",
        "Handler = HandleIrrelevant()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "ZzsXJuayUsk2"
      },
      "outputs": [],
      "source": [
        "# Handler.run(\"What is the trend?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "5NqR7iXE65aV"
      },
      "outputs": [],
      "source": [
        "# tools = [trend, SR, TP, Sl, Bias, databaseId, databaseRisk, databaseStyle, Handler]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "AmVaq852XjDt"
      },
      "outputs": [],
      "source": [
        "tools = [Trend, SR, TP, SL, Bias, Handler]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "mmuJdTx-LkO3"
      },
      "outputs": [],
      "source": [
        "tool_executor = ToolExecutor(tools)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nDgbgr9HdZl"
      },
      "source": [
        "## Helper functions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "j1mr9W5nxLW6"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import AIMessage, BaseMessage, ChatMessage, FunctionMessage, HumanMessage\n",
        "from langchain.tools.render import format_tool_to_openai_function\n",
        "from langchain_core.agents import AgentAction, AgentFinish\n",
        "from langgraph.graph import END, StateGraph\n",
        "from langgraph.prebuilt.tool_executor import ToolExecutor, ToolInvocation\n",
        "from langchain_core.tools import tool\n",
        "from typing import Annotated, List, Sequence, Tuple, TypedDict, Union\n",
        "from langchain.agents import create_openai_functions_agent\n",
        "from langchain_core.prompts import ChatPromptTemplate, BasePromptTemplate, MessagesPlaceholder\n",
        "import operator\n",
        "import functools\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "QDz0tK9PxPDJ"
      },
      "outputs": [],
      "source": [
        "def create_agent(llm, tools, system_message: str):\n",
        "  \"\"\"Create an agent.\"\"\"\n",
        "  functions = [format_tool_to_openai_function(t) for t in tools]\n",
        "  prompt = ChatPromptTemplate.from_messages(\n",
        "      [\n",
        "          (\n",
        "              \"system\",\n",
        "              \" You are a helpful AI assistant, collaborating with other assistants.\"\n",
        "              \" Use the provided tools to progress towards answering the question.\"\n",
        "              \" If you are unable to fully answer, that's OK, another assistant with different tools \"\n",
        "              \" will help where you left off. Execute what you can to make progress.\"\n",
        "              \" If you or any of the other assistants have the final answer or deliverable,\"\n",
        "              \" prefix your response with FINAL ANSWER so the team knows to stop.\"\n",
        "              f\" You have access to the following tools: {tool}.\\n{system_message}\",\n",
        "          ),\n",
        "          MessagesPlaceholder(variable_name=\"messages\"),\n",
        "      ]\n",
        "  )\n",
        "  prompt = prompt.partial(system_message=system_message)\n",
        "  prompt = prompt.partial(tool_names=\", \".join([tool.name for tool in tools]))\n",
        "  return prompt | llm.bind_functions(functions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "qmFMyzFvxkVl"
      },
      "outputs": [],
      "source": [
        "def agent_node(state, agent, name):\n",
        "  result = agent.invoke(state)\n",
        "\n",
        "  if isinstance(result, FunctionMessage):\n",
        "      pass\n",
        "\n",
        "  else:\n",
        "      result = HumanMessage(**result.dict(exclude={\"type\", \"name\"}), name=name)\n",
        "\n",
        "  return {\n",
        "      \"messages\": [result],\n",
        "      \"sender\": name,\n",
        "      \"output_json\": state[\"output_json\"]\n",
        "  }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uG5-7xPrWAEO"
      },
      "source": [
        "## Agents:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "I3GZHhQgWrtm"
      },
      "outputs": [],
      "source": [
        "from langchain_core.agents import AgentActionMessageLog\n",
        "from langchain_openai import AzureChatOpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "LW8US-J3xzUH"
      },
      "outputs": [],
      "source": [
        "api_type = \"azure\"\n",
        "api_endpoint = 'https://tensurfbrain1.openai.azure.com/'\n",
        "api_version = '2023-10-01-preview'\n",
        "api_key = '80ddd1ad72504f2fa226755d49491a61'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "P7L2tpF8xzUH"
      },
      "outputs": [],
      "source": [
        "llm = AzureChatOpenAI(\n",
        "                  api_key=api_key,\n",
        "                  api_version=api_version,\n",
        "                  azure_endpoint=api_endpoint,\n",
        "                  deployment_name=\"gpt_35_16k\",\n",
        "                  temperature=0,\n",
        "                  streaming=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "k9XEZ_r8xscp"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Sepehr\\anaconda3\\envs\\sepehr\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The function `format_tool_to_openai_function` was deprecated in LangChain 0.1.16 and will be removed in 0.3.0. Use langchain_core.utils.function_calling.convert_to_openai_function() instead.\n",
            "  warn_deprecated(\n"
          ]
        }
      ],
      "source": [
        "trading_agent = create_agent(\n",
        "    llm,\n",
        "    [Trend, SR, TP, SL, Bias],\n",
        "    system_message=\"\"\"You are an assistant with the following capabilities given to you by your tools:\n",
        "SR: Calculate support and resistance levels. \\\n",
        "Trend: Calculate the trend of a specified financial instrument over a given time range and timeframe. \\\n",
        "TP: Calculate Take profit (TP), \\\n",
        "SL: Calculate Stoploss (SL), \\\n",
        "Bias: Detecting trading bias. \\\n",
        "Note the following rules for result of each tool: \\\n",
        "SR:Do not mention the name of the parameters of the functions directly in the final answer. Instead, briefly explain them and use other meaningfuly related synonyms. Do not mention the name of the levels that the level is support or resistance. The final answer should also contain the following texts: These levels are determined based on historical price data and indicate areas where the price is likely to encounter support or resistance. The associated scores indicate the strength or significance of each level, with higher scores indicating stronger levels. \\\n",
        "Trend:At any situations, never return the number which is the output of the Trend function. Instead, use its correcsponding explanation which is in the Trend tool's description. Make sure to mention the start_datetime and end_datetime or the lookback parameter if the user have mentioned in their last message. If the user provide neither specified both start_datetime and end_datetime nor lookback parameters, politely tell them that they should and introduce these parameters to them so that they can use them. Do not mention the name of the parameters of the functions directly in the final answer. Instead, briefly explain them and use other meaningfuly related synonyms. Now generate a proper response. \\\n",
        "TP: Do not mention the name of the parameters of the functions directly in the final answer. Instead, briefly explain them and use other meaningfuly related synonyms. Now generate a proper response. \\\n",
        "SL: Do not mention the name of the parameters of the functions directly in the final answer. Instead, briefly explain them and use other meaningfuly related synonyms. The unit of every number in the answer should be mentioned. Now generate a proper response. \\\n",
        "Bias: Do not mention the name of the parameters of the functions directly in the final answer. Instead, briefly explain them and use other meaningfuly related synonyms. Now generate a proper response.\"\"\"\n",
        ")\n",
        "trading_node = functools.partial(agent_node, agent=trading_agent, name=\"Trading\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "DlECe-H-yZaV"
      },
      "outputs": [],
      "source": [
        "# onboarding_agent = create_agent(\n",
        "#     llm,\n",
        "#     [databaseId, databaseRisk, databaseStyle],\n",
        "#     \"\"\"You are an onboarding assistant who helps new users set and change some of the user fields in the database.\n",
        "#        Note that user requests in our systems must only pertain to financial and trading topics.\n",
        "#        If the user’s request is not related to these topics, this tool will handle it.\"\"\",\n",
        "# )\n",
        "# onboarding_node = functools.partial(agent_node, agent=onboarding_agent, name=\"Onboarding\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjyNdJzWW4-C"
      },
      "source": [
        "## Graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elx78emqXsiT"
      },
      "source": [
        "### adding sub graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {
        "id": "Thp0qoYhCrDj"
      },
      "outputs": [],
      "source": [
        "import uuid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5KxGO5t6CobZ"
      },
      "outputs": [],
      "source": [
        "def reduce_list(left: list | None, right: list | None) -> list:\n",
        "  \"\"\"Append the right-hand list, replacing any elements with the same id in the left-hand list.\"\"\"\n",
        "  if not left:\n",
        "      left = []\n",
        "  if not right:\n",
        "      right = []\n",
        "  left_, right_ = [], []\n",
        "  for orig, new in [(left, left_), (right, right_)]:\n",
        "      for val in orig:\n",
        "          if not isinstance(val, dict):\n",
        "              val = {\"val\": val}\n",
        "          if \"id\" not in val:\n",
        "              val[\"id\"] = str(uuid.uuid4())\n",
        "          new.append(val)\n",
        "  # Merge the two lists\n",
        "  left_idx_by_id = {val[\"id\"]: i for i, val in enumerate(left_)}\n",
        "  merged = left_.copy()\n",
        "  for val in right_:\n",
        "      if (existing_idx := left_idx_by_id.get(val[\"id\"])) is not None:\n",
        "          merged[existing_idx] = val\n",
        "      else:\n",
        "          merged.append(val)\n",
        "  return merged"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MB1Z12UJxPLB"
      },
      "outputs": [],
      "source": [
        "# class AgentState(TypedDict):\n",
        "#   messages: Annotated[Sequence[BaseMessage], operator.add]\n",
        "#   path: Annotated[list[str], reduce_list]\n",
        "#   # sender: str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9bdSVwA4iPg"
      },
      "outputs": [],
      "source": [
        "# subGraphOne = StateGraph(AgentState)\n",
        "# mainGraph = StateGraph(AgentState)\n",
        "\n",
        "# mainGraph.add_node(\"Handler\", run_Handler)\n",
        "\n",
        "# subGraphOne.add_node(\"Greeting\", run_greeting)\n",
        "# subGraphOne.add_node(\"Tutorial\", run_tutorial)\n",
        "\n",
        "# subGraphOne.add_edge(\"Greeting\", END)\n",
        "# subGraphOne.add_edge(\"Tutorial\", END)\n",
        "# subGraphOne.add_conditional_edges(\n",
        "#     \"Handler\",\n",
        "#     router,\n",
        "#     {\"Greeting\": \"Greeting\", \"Tutorial\": \"Tutorial\", \"end\": END},\n",
        "# )\n",
        "\n",
        "# subGraphOne.set_entry_point(\"Handler\")\n",
        "# # subGraphOne.set_finish_point(\"Trading\")\n",
        "\n",
        "# mainGraph.add_node(\"Trading\", trading_node)\n",
        "# mainGraph.add_node(\"Onboarding\", onboarding_node)\n",
        "# mainGraph.add_node(\"call_tool\", tool_node)\n",
        "# mainGraph.add_node(\"child\", subGraphOne.compile())\n",
        "\n",
        "# mainGraph.add_conditional_edges(\n",
        "#     \"Trading\",\n",
        "#     router,\n",
        "#     {\"continue\": \"Trading\", \"call_tool\": \"call_tool\", \"end\": END},\n",
        "# )\n",
        "# mainGraph.add_conditional_edges(\n",
        "#     \"Onboarding\",\n",
        "#     router,\n",
        "#     {\"continue\": \"Trading\", \"call_tool\": \"call_tool\", \"end\": END},\n",
        "# )\n",
        "\n",
        "\n",
        "# mainGraph.add_conditional_edges(\n",
        "#     \"call_tool\",\n",
        "#     lambda x: x[\"sender\"],\n",
        "#     {\n",
        "#         \"Trading\": \"Trading\",\n",
        "#         \"Onboarding\": \"Onboarding\",\n",
        "#     },\n",
        "# )\n",
        "# mainGraph.set_entry_point(\"Handler\")\n",
        "\n",
        "# graph = mainGraph.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pXiBnGkX13I"
      },
      "source": [
        "### normal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "tVkwTo3E48dI"
      },
      "outputs": [],
      "source": [
        "def run_Handler(state):\n",
        "  tool_name = \"HandleIrrelevant\"\n",
        "\n",
        "  messages = state[\"messages\"]\n",
        "  last_message = messages[-1]\n",
        "\n",
        "  # print(\"&\"*50)\n",
        "  # print(last_message.content)\n",
        "  # print(\"&\"*50)\n",
        "\n",
        "  action = ToolInvocation(\n",
        "      tool=tool_name,\n",
        "      tool_input=last_message.content,\n",
        "  )\n",
        "\n",
        "  response = tool_executor.invoke(action)\n",
        "  # print(response)\n",
        "  standard_response = \"\"\n",
        "  if response == 'True':\n",
        "    standard_response = \"I'm here to help with trading and financial market queries. If you think your ask relates to trading and isn't addressed, please report a bug using the bottom right panel.\"\n",
        "\n",
        "  function_message = FunctionMessage(\n",
        "       content=standard_response, HandleIrrelevant_response=f\"{str(response)}\", name=action.tool\n",
        "  )\n",
        "\n",
        "  return {\"messages\": [function_message]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "JkjI0fgMbhT2"
      },
      "outputs": [],
      "source": [
        "def run_greeting(state):\n",
        "  # tool_name = \"HandleIrrelevant\"\n",
        "\n",
        "  # action = ToolInvocation(\n",
        "  #     tool=tool_name,\n",
        "  #     tool_input=\"Hi\",\n",
        "  # )\n",
        "\n",
        "  # response = tool_executor.invoke(action)\n",
        "\n",
        "  greeting = ChatWithOpenai(system_message=\"You are an financial and trading assistant.\",\n",
        "                            model=\"gpt_35_16k\",\n",
        "                            temperature=0.2,\n",
        "                            max_tokens=100,\n",
        "                            client=client)\n",
        "  input = [{\"role\": \"user\", \"content\": \"Hi\"}]\n",
        "  response = greeting.chat(input)\n",
        "\n",
        "  function_message = FunctionMessage(\n",
        "       content=response ,name='openai chat'\n",
        "  )\n",
        "\n",
        "  return {\"messages\": [function_message]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "wxNA-zRPTE42"
      },
      "outputs": [],
      "source": [
        "def run_tutorial(state):\n",
        "  response = \"\"\"I'm TenSurf Brain, your AI trading assistant within TenSurf Hub platform, designed to enhance your trading experience with advanced analytical and data-driven tools:\n",
        "1. Trend Detection: I can analyze and report the trend of financial instruments over your specified period. For example, ask me, \"What is the trend of NQ stock from May-1-2024 12:00:00 until May-5-2024 12:00:00?\"\n",
        "2. Support and Resistance Levels: I identify and score key price levels that may influence market behavior based on historical data. Try, \"Calculate Support and Resistance Levels based on YM by looking back up to the past 10 days and a timeframe of 1 hour.\"\n",
        "3. Stop Loss Calculation: I determine optimal stop loss points to help you manage risk effectively. Query me like, \"How much would be the optimal stop loss for a short trade on NQ?\"\n",
        "4. Take Profit Calculation: I calculate the ideal exit points for securing profits before a potential trend reversal. For example, \"How much would be the take-profit of a short position on Dow Jones with the stop loss of 10 points?\"\n",
        "5. Trading Bias Identification: I analyze market conditions to detect the best trading biases and directions, whether for long or short positions. Ask me, \"What is the current trading bias for ES?\"\n",
        "Each tool is tailored to help you make smarter, faster, and more informed trading decisions. Enjoy!\"\"\"\n",
        "\n",
        "  function_message = FunctionMessage(\n",
        "       content=response ,name='hardcoded string'\n",
        "  )\n",
        "\n",
        "  return {\"messages\": [function_message]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "BfUv6qxqbDDg"
      },
      "outputs": [],
      "source": [
        "def output_json_assigner(tool_name, response):\n",
        "  output_json = {}\n",
        "  if tool_name == \"calculate_sr\":\n",
        "    sr_value, sr_start_date, sr_detect_date, sr_end_date, sr_importance = response\n",
        "    output_json[\"levels_prices\"] = sr_value\n",
        "    output_json[\"levels_start_timestamps\"] = sr_start_date\n",
        "    output_json[\"levels_detect_timestamps\"] = sr_detect_date\n",
        "    output_json[\"levels_end_timestamps\"] = sr_end_date\n",
        "    output_json[\"levels_scores\"] = sr_importance\n",
        "  return output_json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "nS7yahoQzOaI"
      },
      "outputs": [],
      "source": [
        "def tool_node(state):\n",
        "  \"\"\"This runs tools in the graph\n",
        "      It takes in an agent action\n",
        "      and calls that tool and\n",
        "      returns the result.\"\"\"\n",
        "  messages = state[\"messages\"]\n",
        "  last_message = messages[-1]\n",
        "  tool_input = json.loads(\n",
        "      last_message.additional_kwargs[\"function_call\"][\"arguments\"]\n",
        "  )\n",
        "\n",
        "  if len(tool_input) == 1 and \"__arg1\" in tool_input:\n",
        "      tool_input = next(iter(tool_input.values()))\n",
        "  # tool_name = last_message.additional_kwargs[\"function_call\"][\"name\"]\n",
        "\n",
        "  tool_name = last_message.additional_kwargs[\"function_call\"][\"name\"].split(\".\")[-1]\n",
        "\n",
        "  action = ToolInvocation(\n",
        "      tool=tool_name,\n",
        "      tool_input=tool_input,\n",
        "  )\n",
        "\n",
        "  response = tool_executor.invoke(action)\n",
        "\n",
        "  out_json = output_json_assigner(tool_name, response)\n",
        "\n",
        "\n",
        "  function_message = FunctionMessage(\n",
        "      content=f\"{tool_name} response: {str(response)}\", name=action.tool\n",
        "  )\n",
        "\n",
        "  return {\"messages\": [function_message], \"output_json\":out_json}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "ZqafxFRLzXlm"
      },
      "outputs": [],
      "source": [
        "def router(state):\n",
        "    messages = state[\"messages\"]\n",
        "    last_message = messages[-1]\n",
        "\n",
        "    if last_message.name != 'HandleIrrelevant':\n",
        "      last_message = messages[-2]\n",
        "\n",
        "      if \"function_call\" in last_message.additional_kwargs:\n",
        "        return \"call_tool\"\n",
        "\n",
        "      if not \"function_call\" in last_message.additional_kwargs and last_message.type == \"function\":\n",
        "        return \"continue\"\n",
        "\n",
        "      if not last_message.additional_kwargs and last_message.type == \"human\":\n",
        "        return \"end\"\n",
        "\n",
        "    else:\n",
        "      if \"Greeting\" in last_message.HandleIrrelevant_response and last_message.name == 'HandleIrrelevant':\n",
        "        return \"Greeting\"\n",
        "\n",
        "      if \"Tutorial\" in last_message.HandleIrrelevant_response and last_message.name == 'HandleIrrelevant':\n",
        "        return \"Tutorial\"\n",
        "\n",
        "      if \"True\" in last_message.HandleIrrelevant_response and last_message.name == 'HandleIrrelevant':\n",
        "        return \"end\"\n",
        "\n",
        "      if \"False\" in last_message.HandleIrrelevant_response and last_message.name == 'HandleIrrelevant':\n",
        "        return \"continue\"\n",
        "\n",
        "      if \"False\" in last_message.content and last_message.name == \"HandleGreeting\":\n",
        "        return \"continue\"\n",
        "\n",
        "      if \"False\" not in last_message.content and last_message.name == \"HandleGreeting\":\n",
        "        return \"end\"\n",
        "\n",
        "\n",
        "    return \"continue\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "rQJEV9IJYO0r"
      },
      "outputs": [],
      "source": [
        "class AgentState(TypedDict):\n",
        "  messages: Annotated[Sequence[BaseMessage], operator.add]\n",
        "  output_json: dict\n",
        "  input_json: dict\n",
        "  sender: str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "HWKy0Tnuzr7P"
      },
      "outputs": [],
      "source": [
        "workflow = StateGraph(AgentState)\n",
        "\n",
        "workflow.add_node(\"Trading\", trading_node)\n",
        "# workflow.add_node(\"Onboarding\", onboarding_node)\n",
        "workflow.add_node(\"call_tool\", tool_node)\n",
        "workflow.add_node(\"Handler\", run_Handler)\n",
        "workflow.add_node(\"Greeting\", run_greeting)\n",
        "workflow.add_node(\"Tutorial\", run_tutorial)\n",
        "\n",
        "# workflow.add_node(\"Inquiry\", inquiry_node)\n",
        "# workflow.add_node(\"Support\", support_node)\n",
        "# workflow.add_node(\"Inquiry\", inquiry_node)\n",
        "\n",
        "# workflow.add_edge(\"Inquiry\", \"call_tool\")\n",
        "\n",
        "# workflow.add_edge(\"Handler\", \"Trading\")\n",
        "\n",
        "workflow.add_edge(\"Greeting\", END)\n",
        "workflow.add_edge(\"Tutorial\", END)\n",
        "\n",
        "# workflow.add_conditional_edges(\n",
        "#     \"Inquiry\",\n",
        "#     router,\n",
        "#     {\"call_tool\": \"call_tool\", \"continue\": \"Inquiry\"},\n",
        "# )\n",
        "# workflow.add_conditional_edges(\n",
        "#     \"Support\",\n",
        "#     router,\n",
        "#     {\"continue\": \"Trading\", \"call_tool\": \"call_tool\", \"end\": END},\n",
        "# )\n",
        "workflow.add_conditional_edges(\n",
        "    \"Handler\",\n",
        "    router,\n",
        "    {\"continue\": \"Trading\", \"Greeting\": \"Greeting\", \"Tutorial\": \"Tutorial\", \"end\": END},\n",
        ")\n",
        "# workflow.add_conditional_edges(\n",
        "#     \"Greeting\",\n",
        "#     router,\n",
        "#     {\"continue\": \"Trading\", \"end\": END},\n",
        "# )\n",
        "workflow.add_conditional_edges(\n",
        "    \"Trading\",\n",
        "    router,\n",
        "    {\"continue\": \"Trading\", \"call_tool\": \"call_tool\", \"end\": END},\n",
        ")\n",
        "# workflow.add_conditional_edges(\n",
        "#     \"Onboarding\",\n",
        "#     router,\n",
        "#     {\"continue\": \"Trading\", \"call_tool\": \"call_tool\", \"end\": END},\n",
        "# )\n",
        "\n",
        "\n",
        "workflow.add_conditional_edges(\n",
        "    \"call_tool\",\n",
        "    lambda x: x[\"sender\"],\n",
        "    {\n",
        "        \"Trading\": \"Trading\",\n",
        "        # \"Onboarding\": \"Onboarding\",\n",
        "        # \"Inquiry\": \"Inquiry\",\n",
        "        # \"Support\": END\n",
        "    },\n",
        ")\n",
        "workflow.set_entry_point(\"Handler\")\n",
        "\n",
        "graph = workflow.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8B2R2gjXU6A"
      },
      "source": [
        "## Graph visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {
        "id": "MogS2xefyYIf"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, HTML\n",
        "import base64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {
        "id": "-3izld86Xfj9"
      },
      "outputs": [],
      "source": [
        "def display_image(image_bytes: bytes, width=300):\n",
        "    decoded_img_bytes = base64.b64encode(image_bytes).decode(\"utf-8\")\n",
        "    html = f'<img src=\"data:image/png;base64,{decoded_img_bytes}\" style=\"width: {width}px;\" />'\n",
        "    display(HTML(html))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "OlsVc0H1xviv",
        "outputId": "a7442c6f-6d56-41c6-d15e-2a29d9c552ca"
      },
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "Install pygraphviz to draw graphs: `pip install pygraphviz`.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "File \u001b[1;32mc:\\Users\\Sepehr\\anaconda3\\envs\\sepehr\\lib\\site-packages\\langchain_core\\runnables\\graph_png.py:136\u001b[0m, in \u001b[0;36mPngDrawer.draw\u001b[1;34m(self, graph, output_path)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 136\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpygraphviz\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpgv\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[import]\u001b[39;00m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pygraphviz'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[158], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m display_image(\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw_png\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
            "File \u001b[1;32mc:\\Users\\Sepehr\\anaconda3\\envs\\sepehr\\lib\\site-packages\\langchain_core\\runnables\\graph.py:401\u001b[0m, in \u001b[0;36mGraph.draw_png\u001b[1;34m(self, output_file_path, fontname, labels)\u001b[0m\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrunnables\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraph_png\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PngDrawer\n\u001b[0;32m    388\u001b[0m default_node_labels \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    389\u001b[0m     node\u001b[38;5;241m.\u001b[39mid: node_data_str(node) \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnodes\u001b[38;5;241m.\u001b[39mvalues()\n\u001b[0;32m    390\u001b[0m }\n\u001b[0;32m    392\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPngDrawer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    393\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfontname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    394\u001b[0m \u001b[43m    \u001b[49m\u001b[43mLabelsDict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    395\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[0;32m    396\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdefault_node_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    397\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnodes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    398\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    399\u001b[0m \u001b[43m        \u001b[49m\u001b[43medges\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43medges\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    400\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m--> 401\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Sepehr\\anaconda3\\envs\\sepehr\\lib\\site-packages\\langchain_core\\runnables\\graph_png.py:138\u001b[0m, in \u001b[0;36mPngDrawer.draw\u001b[1;34m(self, graph, output_path)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpygraphviz\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpgv\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[import]\u001b[39;00m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m--> 138\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m    139\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInstall pygraphviz to draw graphs: `pip install pygraphviz`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    140\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;66;03m# Create a directed graph\u001b[39;00m\n\u001b[0;32m    143\u001b[0m viz \u001b[38;5;241m=\u001b[39m pgv\u001b[38;5;241m.\u001b[39mAGraph(directed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, nodesep\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m, ranksep\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n",
            "\u001b[1;31mImportError\u001b[0m: Install pygraphviz to draw graphs: `pip install pygraphviz`."
          ]
        }
      ],
      "source": [
        "display_image(graph.get_graph().draw_png())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FWNCO4ZXlMp"
      },
      "source": [
        "## Test prompts:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# sample prompt for each function \n",
        "prompts = [ \n",
        "    # detect_trend \n",
        "    \"What is the trend of NQ stock from May-1-2024 12:00:00 until May-5-2024 12:00:00?\", \n",
        "    # calculate_sr \n",
        "    \"Calculate Support and Resistance Levels based on YM by looking back up to past 10 days and timeframe of 10 minutes.\", \n",
        "    # calculate_sl \n",
        "    \"What would be the stop loss of trading short positinos based on NQ and minmax method by looking back up to 30 candles?\", \n",
        "    # calculate_tp \n",
        "    \"How much would be the take-profit of the NQ with the stop loss of 10 and direction of 1?\", \n",
        "    # bias_detection \n",
        "    \"Tell me about the bias of ES on the market.\",\n",
        "\t# irrelevant\n",
        "\t\"What are the rules of basketball?\"\n",
        "]\n",
        "\n",
        "for prompt in prompts: \n",
        "\tprint(f\"Prompt:  \t{prompt}\") \n",
        "\tfor s in graph.stream( \n",
        "\t\t\t{ \n",
        "\t\t\t\t\"messages\": [ \n",
        "\t\t\t\t\tHumanMessage( \n",
        "\t\t\t\t\tcontent=prompt \n",
        "\t\t\t\t\t) \n",
        "\t\t\t\t], \n",
        "\t\t\t\t\"input_json\": input_filter.front_end_json_sample \n",
        "\t\t\t}, \n",
        "\t\t\t# Maximum number of steps to take in the graph \n",
        "\t\t\t{\"recursion_limit\": 150}, \n",
        "\t\t): \n",
        "\t\tprint(s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "nlPLaNe4GjgH",
        "behgQf7V1wNk",
        "EciRRD4YELCo",
        "lt4sSmAQH1Ph",
        "AahZLwrkH-G-",
        "ALEtGCoWoI_2",
        "LwjkTlbXlsgC",
        "elx78emqXsiT",
        "1FWNCO4ZXlMp"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
